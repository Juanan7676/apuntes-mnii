
\section{Introducción. Orden de convergencia}
\label{sec:intro-orden-convergencia}

En matemáticas y en disciplinas relacionadas con el cálculo científico
aparece con frecuencia el problema de hallar los \textit{ceros de una
  función}.
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[ \axisXYmiddle, xtick=\empty, ytick=\empty, legend
      pos = south east ]
      % Draw a curve
      \addplot[domain=-1:3, blue, ultra thick] {-(x+3)*(x-1)*(x-5)};
      % Plot a label at curve root
      \node[coordinate, medium dot, pin=-30:{$\cero$}] at (axis
      cs:1,0) {};
      % Draw the name of curve
      \legend {$f$};
    \end{axis}
  \end{tikzpicture}
\end{center}

Recordemos que, dada una función real de una variable real, $f$, con
dominio $I\subset\Rset$, una \textit{raíz} (o un \textit{cero}) de $f$
es una solución del siguiente problema:
\begin{equation}
  \label{eq:raiz}
  \tag{P}
  \text{Hallar $\cero\in I$ tal que} \quad f(\cero)=0.
\end{equation}

% El cero es simple si
% $f'(\cero)=0$ y múltiple si $f'(\cero)=0$.

Este no es un problema sencillo y, salvo en algunos casos concretos no
disponemos de algoritmos que nos permitan obtener raíces de una
ecuación en un número finito de pasos. Por ejemplo, no existen
fórmulas explícitas para hallar ceros de funciones polinómicas
arbitrarias de grado $n\ge 5$ y el problema es aún más difícil si $f$
no es polinómica. En general, no podemos plantearnos el hallar las
raíces exactas de una ecuación. Más aún cuando en numerosos <<problemas
reales>>, esta función es conocida sólo de forma aproximada.

Recurriremos a métodos numéricos que, usualmente, vendrán dados en
forma de \textit{algoritmos iterativos}, es decir, partiendo de uno o
más datos iniciales, intentaremos construir una sucesión
$\{x_k\}_{k=0}^{\infty}$ tal que
$$
x_k \to \cero,
$$
done $\cero$ es una raíz de $f$. La idea es elegir $N\in\Nset$
<<suficientemente grande>> de forma que $x_N$ sea <<una buena
aproximación>> de $\cero$; escribiremos
$$
\cero\approx x_N.
$$

El \textit{error de truncamiento} (o simplemente, el
\textit{error}) del método en la etapa $k$ se define como:
$$
e_k = |x_k - \alpha|.
$$

Nuestro objetivo es estudiar algoritmos eficientes que nos permitan
determinar de forma aproximada los ceros de una función $f$ en un
intervalo $I=[a,b]$, de tal forma que el error de truncamiento
respecto a la solución exacta sea tan pequeño como deseemos.

Como veremos, no existe <<\textit{el mejor método}>> para el cálculo
de ceros de funciones: algunos son más rápidos, otros requieren una
buena estimación inicial de la raíz, o regularidad en la función... En
general, un buen método será \emph{muy general} (es decir, podrá
utilizarse para un rango de funciones muy amplio) y a la vez
\emph{poco costoso} (exigirá pocos recursos de ordenador hacer pequeño
el error con la solución exacta). En general identificaremos los
recursos computacionales con el número de operaciones requeridas por
el algoritmo\footnote{También con la cantidad de memoria en el
  ordenador utilizada, aunque en los algoritmos para resolución de
  ecuaciones 1D suelen poco exigente en cuanto a requerimientos de
  memoria.}. Pero en general, la naturaleza de la función $f$ se
escapa de nuestro control y no conoceremos cuantas operaciones
requiere su evaluación.  Por tanto, será frecuente el \emph{estimar el
  coste del algoritmo en términos del número de evaluaciones de $f$}
(y no del número de operaciones en coma flotante).

% En todo caso, el precio de un aumento de los recursos empleados por un
% algoritmo nos puede compensar si, a cambio, aumenta el orden de
% convergencia.


\subsection*{Orden de convergencia}

Consideremos un método iterativo definido por una sucesión $\{x_k\}$
tal que $\lim x_k=\alpha$.
Para medir la ``rapidez con la que converge el método''
utilizaremos el concepto de orden de convergencia.

\begin{definition}
  \label{def:orden-convergencia}
  Sea $\{x_k\}$ una sucesión convergente a $\alpha$ y sea $p\ge
  1$.
  Decimos que la sucesión tiene \resaltar{orden de convergencia
    exactamente igual} a $p\ge 1$ si existe una constante $C>0$ (y
  $C<1$ si $p=1$) tal que
  \begin{equation}
    \label{eq:orden-convergencia}
    \lim_{k\to+\infty} \frac{e_{k+1}}{e_k^p} = C.
  \end{equation}
  En este caso, a $C$ se le llama constante asintótica de error.
\end{definition}

\begin{remark}
  Asumiendo que $x_k \to \alpha$, tendremos que $e_k \to 0$ (y en
  particular existe $k_o$ tal que $0<e_k<1$, para todo $k\ge
  k_o$).
  Cuando La ecuación~(\ref{eq:orden-convergencia}) se puede
  interpretar en el siguiente sentido: en cada iteración, el error
  disminuye como la potencia $p$ del error en la iteración anterior.
  Este hecho lo podemos escribir, de forma rigurosa, como:
  \begin{equation}
    \label{eq:orden-convergencia-aprox}
    e_{k+1} \approx C e_k^p
  \end{equation}
  (en el caso en el que $p=1$, imponemos que $C<1$ para que la
  sucesión de errores tienda a cero).

  Para dar un sentido más preciso a la
  expresión~(\ref{eq:orden-convergencia-aprox}), obsérvese que una
  sucesión con orden de convergencia $p$
  verifica~\eqref{eq:orden-convergencia}, es decir que para todo
  $\varepsilon>0$ existe $k_\epsilon\in\Nset$ tal que si
  $k\ge k_\epsilon$ se tiene
  ${e_{k+1}}/{e_k^p} \in (C-\varepsilon,C+\varepsilon)$.  Por tanto, el
  error en la etapa $k+1$ está acotado superior e inferiormente por el
  error en la etapa $k$:
    \begin{align*}
      (C-\varepsilon) e_k^p < e_{k+1} <
      (C+\varepsilon)  e_k^p, \quad \forall k \ge k_\epsilon.
    \end{align*}
    Nótese que $(C-\varepsilon)$ y $(C+\varepsilon)$ convergen a $C$
    cuando $\varepsilon\to 0$.
    % La desigualdad anterior significa que (fijando la notación
    % $C^\flat_\varepsilon=C-\varepsilon$ y
    % $C^\sharp_\varepsilon=C+\varepsilon$), el error en la etapa $k+1$
    % está acotado inferior y superiormente por el error en la etapa $k$
    % de la siguiente forma:
    % \begin{align*}
    %   C^\flat_\varepsilon \; e_k^p \le e_{k+1},\\
    %   e_{k+1} \le C^\sharp_\varepsilon \; e_k^p.
    % \end{align*}
\end{remark}

En el caso $p=1$ decimos que la convergencia es (exactamente)
\textit{lineal}. Los casos $p>1$ se llaman de convergencia
\textit{superlineal} (cuadrática para $p=2$, supercuadrática si $p>2$,
cúbica para $p=3$, etc.).

\begin{example}
  \label{rk:2}
  Es muy sencillo comprobar que la sucesión $x_k=2^{-k}$ converge a cero
  con orden de convergencia lineal ($p=1$), con constante asintótica $C=1/2$.
\end{example}


% el que exige $C<1$ para garantizar la disminución del error)
Pero en general, el determinar de forma exacta el orden de
convergencia de un algoritmo no es una tarea trivial. Por suerte,
habitualmente, no será necesario hacerlo y bastará con calcular el
orden en un sentido más débil.

En concreto, es fácil comprobar que si una sucesión tiene orden de
convergencia exactamente igual a $p$, entonces existen una constante a
la que también llamaremos $C>0$ y $k_0\in\Nset$ tales que
\begin{equation}
  \label{eq:orden-convergencia-al-menos-p}
  e_{k+1} \le C e_k^p \quad \forall k\ge k_0.
\end{equation}

\begin{definition}
  Diremos que un método iterativo (definido por una sucesión $\{x_k\}$
  tal que $\lim x_k=\alpha$) tiene \resaltar{orden de convergencia
    mayor o igual a} $p\ge 1$ si existen una constante $C>0$ y un
  número entero $k_0>0$ para los que se
  verifica~(\ref{eq:orden-convergencia-al-menos-p})
  \label{def:orden-convergencia-al-menos-p}
\end{definition}


Algunas propiedades (cuya demostración se deja como ejercicio):
\begin{itemize}
\item Las definiciones anteriores son consistentes en el sentido de
  que una sucesión que converja con orden exactamente igual a $p$ (es
  decir, verifica~(\ref{eq:orden-convergencia})) convergerá también
  con mayor o igual a $p$ (es decir,
  verifica~(\ref{eq:orden-convergencia-al-menos-p}).
\item Un método iterativo de orden exactamente $p$ es de orden mayor o
  igual a $q$, para todo $1\le q < p$ ($C<1$ si $q=1$).
\item La convergencia con orden mayor o igual a $p$ no implica la
  convergencia con un orden exactamente igual a $p$.
\item Análogamente a la
  observación~\ref{rk:interpretacion-orden-convergencia}, la
  desigualdad~(\ref{eq:orden-convergencia-al-menos-p}) garantiza que
  en un esquema de orden (al menos) $p$ tiene lugar el decrecimiento del
  error absoluto en términos de su potencia $p$ (o del factor
  $C<1$ si $p=1$).
\end{itemize}



% \begin{definition}
%   Diremos que un método iterativo (definido por una sucesión $\{x_k\}$ tal
%   que $\lim x_k=\alpha$) tiene \resaltar{orden de convergencia (al menos)}
%   $p\ge 1$ si existen una constante $C>0$ y un número entero $k_0>0$
%   tal que
%   \begin{equation}
%     \label{eq:orden-convergencia-al-menos-p}
%     e_{k+1} \le C e_k^p \quad \forall k\ge k_0.
%   \end{equation}
%   \label{def:orden-convergencia-al-menos-p}
% \end{definition}

% % \begin{remark}
% %   La desigualdad~(\ref{eq:orden-convergencia-2}) significa que todo
% %   método iterativo con orden de convergencia (exactamente) $p$ tiene
% %   orden de convergencia al menos $p$ para una constante ligeramente
% %   mayor, $C+\epsilon$.
% Algunos comentarios:
% \begin{itemize}
% \item Un método iterativo de orden (al menos) $p$ podría tener,
%   exactamente, un orden mayor que $p$. Visto de otra forma, un método
%   iterativo de orden exactamente $p$ es de orden (al menos) $q$, para
%   todo $1\le q < p$ ($C<1$ si $q=1$).
%   \begin{flushright}
%     \vspace{-0.75em}
%     \scriptsize \em La demostración de lo anterior se deja como ejercicio.
%     \vspace{-0.75em}
%   \end{flushright}
% \item En relación con lo anterior, aunque la sucesión $x_k$
%   verifique la desigualdad~(\ref{eq:orden-convergencia-al-menos-p}),
%   no tiene por qué existir el límite~(\ref{eq:orden-convergencia}). En
%   concreto de~(\ref{eq:orden-convergencia-al-menos-p}) sólo podemos
%   concluir que
%   \begin{equation*}
%     \limsup_{k\to+\infty} \frac{e_{k+1}}{e_k^p} < +\infty.
%   \end{equation*}
%   De hecho, no es difícil comprobar que la condición anterior es no
%   solo necesaria, sino suficiente para que la sucesión $x_{k}$ tenga
%   orden (al menos) $p$.
% \item Análogamente a la
%   observación~\ref{rk:interpretacion-orden-convergencia}, la
%   desigualdad~(\ref{eq:orden-convergencia-al-menos-p}) garantiza que
%   en un esquema de orden (al menos) $p$ tiene lugar el decrecimiento del
%   error absoluto en términos de su potencia $p$ (o del factor
%   $C<1$ si $p=1$).
% \end{itemize}

El resto de este capítulo se estructura de la siguiente forma: En una
primera sección, repasamos algunos resultados teóricos para asegurar
la existencia y unicidad de solución de~(\ref{eq:raiz}). En el resto
de las secciones se estudian distintos métodos iterativos para el
cálculo de ceros de funciones de una variable, partiendo del que es,
conceptualmente, más sencillo (el método de Bisección) y llegando
hasta el método de Newton y algunas de sus variantes.  La última
sección ofrece algunas indicaciones sobre la generalización de estos
métodos para resolver sistemas de ecuaciones no lineales.

\section{Existencia y unicidad de solución. Separación de ceros}
\label{sec:tema1:exist-y-unic}

Antes abordar la resolución numérica de cualquier problema es
fundamental el realizar, en una etapa previa, un análisis del mismo
que nos permita determinar la existencia de solución. En caso
afirmativo, en la mayor parte de los casos será muy importante
asegurarnos de que esta solución es única\footnote{Por ejemplo,
  la falta de unicidad de sistemas de ecuaciones lineales está
  asociada con la singularidad de las matrices asociadas. O, en
  métodos iterativos, la no unicidad puede dar pie a
  oscilaciones espurias, es decir, a sucesiones oscilantes que no
  converjan a una solución.}.

Con frecuencia, el proceso previo para determinar los ceros de una
función consistirá en localizar intervalos en los que podamos
garantizar la existencia de una única solución (proceso que recibe el
nombre de \textit{separación de ceros o de soluciones}). Los
siguientes resultados nos proporcionan unas herramientas muy útiles
para ello.

También nos resultará de utilidad, como primera aproximación, la
representación gráfica de la función, aunque \textit{en ningún caso
  debemos confiar exclusivamente en las gráficas generadas por
  programas informáticos}: éstas deben estar respaldadas por un
análisis numérico que garantice la existencia y unicidad de solución.

\begin{theorem}[Bolzano]
  \label{thm:bolzano}
  Sea $f:[a,b]\subset \Rset\to\Rset$ una función continua en el
  intervalo $[a, b]$ y supongamos que $f (a)\cdot f (b) < 0$.
  Entonces, existe $c\in(a, b)$ tal que $f (c) = 0$.
\end{theorem}

\begin{theorem}[Rolle]
  \label{thm:rolle}
  Sea $f:[a,b]\subset \Rset\to\Rset$ una función continua en $[a, b]$ y derivable en
  $(a, b)$ tal que $f(a) = f(b)$.
  Entonces existe al menos un valor $c \in (a, b)$ tal que $f'(c) = 0$.
\end{theorem}

% \begin{remark}[Sobre unicidad en los teoremas de Bolzano y Rolle]
%   \label{rk:tema1:unicidad-bolzano-rolle}
Estos teoremas son bien conocidos en el ámbito del análisis
matemático. El teorema de Bolzano proporciona existencia de solución,
pero no unicidad (es decir, una función continua con distinto signo en
$a$ y en $b$ podría tener muchos ceros en $(a,b)$). El teorema de
Rolle puede ser utilizado para obtener el siguiente resultado de
unicidad:

% \end{remark}

% Combinando ambos teoremas, podemos deducir el siguiente resultado de
% existencia y unicidad:

\begin{corollary}
  \label{cor:tema1:exist+unic}
  Sea $f:[a,b]\subset \Rset\to\Rset$ continua en $[a, b]$ y derivable
  en $(a, b)$. Si $f'(x)\ne 0$, para todo $x\in (a, b)$, entonces
  existe a lo sumo una solución de~\eqref{eq:raiz} en el intervalo
  $I=[a,b]$.
\end{corollary}

\begin{proof}
  Por reducción al absurdo, si existieran dos soluciones
  de~\eqref{eq:raiz}, $\cero_1$ y $\cero_2$ (con $\cero_1<\cero_2$),
  tendríamos que $0=f(\cero_1)=f(\cero_2)$. Puesto que $f$ se
  encuentra en las hipótesis del teorema de Rolle en el subintervalo
  $[\cero_1,\cero_2]$, esto implicaría que existe $c$ entre $\cero_1$
  y $\cero_2$ en el que $f'(c)=0$, lo que contradice las hipótesis.
\end{proof}

\begin{remark}
  \label{rk:tema1:exist+unic}

  El teorema anterior puede generalizarse a intervalos de la forma
  $(-\infty,b]$, $[a,+\infty)$ o $(-\infty,+\infty)$. En efecto, en
  estos casos podríamos repetir la demostración anterior (si
  existieran dos raíces, $\cero_1<\cero_2$, el teorema de Rolle
  implicaría que existe $c\in (\cero_1,\cero_2)$ tal que $f'(c)=0$).
\end{remark}

En realidad, el corolario anterior se basa en el hecho de que (como
consecuencia del Teorema de Rolle), entre dos raíces de $f$ hay al
menos una raíz de $f'$.

\begin{remark}[Separación de ceros]
  \label{rk:tema1:separac-ceros}
  % algoritmo para la separación de
  % soluciones de~\eqref{eq:raiz} que será utilizado en los próximos
  % ejemplos.
  Suponiendo continuidad y derivabilidad de $f$ (para
  aplicar el Corolario~\ref{cor:tema1:exist+unic}), % suponemos que la
  % derivada, $f'$, \emph{es continua en $(a,b)$},
  podemos enunciar un algoritmo para la localización de las raíces de
  $f$:
  \begin{enumerate}
  \item Hallar todas las raíces de $f'$, a las que llamaremos
    $\beta_1<\dots<\beta_n$.
  \item Considerar los intervalos $(a,\beta_1)$, $(\beta_1,\beta_2)$,
    $(\beta_2,\beta_3)$, \dots,$(\beta_n,b)$. Puesto que, dentro de
    ellos, $f'(x)\neq 0$, sabemos (según el Corolario~\ref{cor:tema1:exist+unic})
    que existe, a lo sumo, una raíz de $f$ en cada uno de estos
    intervalos.
  \item Utilizar el teorema de Bolzano para detectar en cuales de
    estos intervalos existe realmente una raíz de $f$.
  \end{enumerate}
  El proceso anterior es válido incluso si $a=-\infty$ o $b=+\infty$,
  según la observación~\ref{rk:tema1:exist+unic}.  Sin embargo, aunque es
  útil desde el punto de vista teórico y en algunos casos
  particulares, presenta un importante inconveniente en la práctica:
  la necesidad de determinar las raíces de  $f'$.  En
  realidad, estamos convirtiendo el problema del cálculo de ceros de
  $f$ en otro similar, el cálculo de ceros de $f'$, que podría ser tan
  complicado como el anterior (o más aún).

  Con frecuencia, se utilizan otros razonamientos para determinar
  todos los intervalos, $(\beta_i,\beta_{i+1})$ en los que
  $f(\beta_i)\cdot f(\beta_{i+1})<0$. Por ejemplo, procedimientos de
  tipo bisección (que analizaremos en la próxima sección).
\end{remark}

A continuación, veremos algunos ejemplos en los que se aplican los
resultados anteriores:

\begin{example}
  \label{ex:tema1:separ-soluc-1}
  Nos planteamos el hallar las raíces de la ecuación
  $$
  f(x)=x^3-9x+3 =0.
  $$
  Siguiendo los pasos de la observación~\ref{rk:tema1:separac-ceros},
  calculamos la derivada de $f$,
  $$
  f'(x)=3x^2-9,
  $$
  que sólo se anula en $x=-\sqrt 3,
  x=+\sqrt 3$. Por lo tanto existen, a lo sumo, tres ceros de $f$ que
  están localizados en
  $$
  (-\infty,-\sqrt 3), (-\sqrt 3, +\sqrt 3) \text{ y } (+\sqrt 3,
  +\infty).
  $$

  Solamente resta utilizar el teorema de Bolzano para detectar en
  cuáles de estos intervalos existe una raíz de $f(x)=0$. Como apoyo,
  podemos utilizar un entorno informático para representar la gráfica
  (figura~\ref{fig:tema1:ejemplo-separ-soluc-1}), que en este caso nos
  sugiere que las raíces pueden ser localizadas, concretamente, en los
  intervalos $[-4,-3]\subset (-\infty,-\sqrt 3)$,
  $[0,1]\subset (-\sqrt 3, +\sqrt 3)$ y
  $[2,3]\subset (+\sqrt 3, +\infty)$.
  \begin{figure}
    \begin{graficaTikz}[width=23em, height=15em]
      \begin{axis}[\axisXYmiddle]
        % Draw a curve
        \addplot[domain=-4.0:4.0, blue, ultra thick, samples=40]
        {x^3-9*x+3};
        % Plot a label at curve root
        \node[coordinate, medium dot, pin=95:{$\cero_1$}] at
        (axis cs:-3.15,0) {};
        \node[coordinate, medium dot, pin=85:{$\cero_2$}] at
        (axis cs:0.33,0) {};
        \node[coordinate, medium dot, pin=95:{ $\cero_3$}] at
        (axis cs:2.81,0) {};
      \end{axis}
    \end{graficaTikz}
    \caption{Gráfica de $f(x)=x^3-9x+3$}
    \label{fig:tema1:ejemplo-separ-soluc-1}
  \end{figure}
  Este hecho lo podemos confirmar utilizando el teorema de Bolzano:
  \begin{itemize}
  \item En $[-4,-3]$: $f(-4)=-25$ y $f(-3)=3$, por lo tanto existe (al
    menos) un valor   $\cero_1 \in (-4,3)$ tal que $f(\cero_1)=0$.
  \item En $[0,1]$: $f(0)=3$ y $f(1)=-5$, luego existe
    $\cero_2 \in (0,1)$ tal que $f(\cero_2)=0$.
  \item En $[2,3]$: $f(2)=-7$ y $f(3)=3$, por tanto existe
    $\cero_3 \in (2,3)$ tal que $f(\cero_3)=0$.
  \end{itemize}
\end{example}

\begin{example}
  Determinaremos el número de valores $x\in\Rset$ tales que
  $2x=cos(x)$ y localizaremos estos valores en intervalos (lo que, en
  las próximas secciones, se usará para aplicar métodos numéricos para
  aproximar las soluciones).

  Para ello, planteamos el problema~\eqref{eq:raiz} para la función $f$
  (continua y derivable) definida por
  $$
  f(x)=2x-\cos(x)
  $$
  y, siguiendo las ideas de la
  observación~\ref{rk:tema1:separac-ceros}, comenzamos estudiando la
  derivada
  $$
  f'(x)=2+\sen(x).
  $$
  Puesto que $|\sen(x)|\le 1$, tenemos $f'(x)\neq 0$ para todo
  $x\in\Rset$, luego existe, como máximo, una raíz de $f$ en
  $(-\infty,+\infty)$.

  \begin{figure}
    \begin{graficaTikz}[width=23em, height=15em]
      \begin{axis}[\axisXYmiddle]
        % Draw a curve
        \addplot[domain=-pi:pi+0.3, blue, ultra thick, samples=40]
        {{2*x-cos(deg(x))}};
        % Plot a label at curve root
        \node[coordinate, medium dot, pin=-87:{$\cero$}] at (axis cs:0.45,0) {};
      \end{axis}
    \end{graficaTikz}
    \caption{Gráfica de $f(x)=2x-\cos(x)$}
    \label{fig:tema1:ejemplo-separ-soluc-2}
  \end{figure}
  La gráfica de $f$ (figura~\ref{fig:tema1:ejemplo-separ-soluc-2}) nos
  sugiere que esta raíz, $\alpha$, es positiva. Por ejemplo si
  aplicamos el teorema de bolzano en el intervalo $[0,\pi/2]$ (elegido
  por resultar sencilla la evaluación de $f$) tenemos: $f(0)=-1$ y
  $f(\pi/2)=\pi$.  Así, $x=\alpha$ es el único valor, concretamente
  localizado en $(0,\pi/2)$, tal que $2x=\cos(x)$.
\end{example}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../tema1.tex"
%%% End:
