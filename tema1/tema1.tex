
\chapter[Raíces de funciones de una variable]{Raíces de funciones de una variable%
  \footnote{\licenseInfo}}
\label{cha:ecuaciones-una-variable}

\section{Introducción. Orden de convergencia}
\label{sec:intro-orden-convergencia}

En matemáticas y en disciplinas relacionadas con el cálculo científico
aparece con frecuencia el problema de hallar los \textit{ceros de una
  función}.
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[ \axisXYmiddle, xtick=\empty, ytick=\empty, legend
      pos = south east ]
      % Draw a curve
      \addplot[domain=-1:3, blue, ultra thick] {-(x+3)*(x-1)*(x-5)};
      % Plot a label at curve root
      \node[coordinate, medium dot, pin=-30:{$\cero$}] at (axis
      cs:1,0) {};
      % Draw the name of curve
      \legend {$f$};
    \end{axis}
  \end{tikzpicture}
\end{center}

Recordemos que, dada una función real de una variable real, $f$, con
dominio $I\subset\Rset$, una \textit{raíz} (o un \textit{cero}) de $f$
es una solución del siguiente problema:
\begin{equation}
\label{eq:raiz}
\tag{P}
\text{Hallar $\cero\in I$ tal que} \quad f(\cero)=0.
\end{equation}

% El cero es simple si
% $f'(\cero)=0$ y múltiple si $f'(\cero)=0$.

Este no es un problema sencillo y, salvo en algunos casos concretos no
disponemos de algoritmos que nos permitan obtener raíces de una
ecuación en un número finito de pasos. Por ejemplo, no existen
fórmulas explícitas para hallar ceros de funciones polinómicas
arbitrarias de grado $n\ge 5$ y el problema es aún más difícil si $f$
no es polinómica. En general, no podemos plantearnos el hallar las
raíces exactas de una ecuación. Más aún cuando en numerosos <<problemas
reales>>, los coeficientes son conocidos sólo de forma aproximada.

Recurriremos a métodos numéricos que, usualmente, vendrán dados en
forma de \textit{algoritmos iterativos}, es decir, partiendo de uno o
más datos iniciales, intentaremos construir una sucesión
$\{x_k\}_{k=0}^{\infty}$ tal que
$$
x_k \to \cero,
$$
done $\cero$ es una raíz de $f$. La idea es elegir $N\in\Nset$
<<suficientemente grande>> de forma que $x_N$ sea <<una buena
aproximación>> de $\cero$; escribiremos
$$
\cero\approx x_N.
$$

Nuestro objetivo es estudiar algoritmos eficientes que nos permitan
determinar de forma aproximada los ceros de una función $f$ en un
intervalo $I=[a,b]$, de tal forma que el error respecto a la solución
exacta sea tan pequeño como deseemos. Como veremos, no existe
<<\textit{el mejor método}>> para el cálculo de ceros de funciones:
algunos son más rápidos, otros requieren una buena estimación inicial
de la raíz, o regularidad en la función... En general, un buen método
será muy general (es decir, podrá utilizarse para un rango de
funciones muy amplio) y a la vez poco costoso (exigirá pocos recursos
de ordenador hacer pequeño el error con la solución exacta). En el
caso de funciones de una variable, la memoria consumida en el
ordenador será pequeña y los recursos empleados se pueden identificar
con el tiempo de cálculo. Habitualmente, puesto que la naturaleza de
la función $f$ se escapa de nuestro ámbito, no podremos estimar con
exactitud el tiempo de cálculo (tampoco la memoria consumida). Por
ello, estimaremos el coste del algoritmo, más que en función del
número de operaciones en coma flotante, en términos del número de
evaluaciones de $f$. En todo caso, el precio de un aumento de los
recursos empleados por un algoritmo nos puede compensar si, a cambio,
aumenta el orden de convergencia.

\subsection*{Orden de convergencia}

En general, un método iterativo converge más rápidamente cuanto mayor
es su orden de convergencia:

\begin{definition}
  \label{def:orden-convergencia}  
  Diremos que un método iterativo (definido por una sucesión $\{x_k\}$
  tal que $\lim x_k=\alpha$) tiene \resaltar{orden de convergencia
    (exactamente)} $p\ge 1$ si existe una constante $C>0$ (y $C<1$ si
  $p=1$) tal que
  \begin{equation}
    \label{eq:orden-convergencia}
    \lim_{k\to+\infty} \frac{|x_{k+1}-\alpha|}{|x_k-\alpha|^p} = C.
  \end{equation}
\end{definition}
A $C$ se le llama constante asintótica de error. En el caso $p=1$ 
decimos que la convergencia es \textit{lineal}. Los casos $p>1$ se
llaman de convergencia \textit{superlineal} (cuadrática para $p=2$,
supercuadrática si $p>2$, cúbica para $p=3$, etc.).

Obsérvese que~\eqref{eq:orden-convergencia} implica que, cuando $x_k$
esté suficientemente cerca de $\alpha$,
\begin{equation}
  \label{eq:orden-convergencia-aprox}
   |x_{k+1}-\alpha| \approx C |x_{k}-\alpha|^p,
\end{equation}
en el sentido de que para todo $\varepsilon>0$ existe $k_0\in\Nset$ tal que
\begin{equation*}
   \label{eq:orden-convergencia-2}
  (C-\varepsilon) |x_{k}-\alpha|^p < |x_{k+1}-\alpha| < 
  (C+\varepsilon)  |x_{k}-\alpha|^p, \quad \forall k \ge k_0.
\end{equation*}

\begin{remark}
\label{rk:interpretacion-orden-convergencia}
  La expresión~(\ref{eq:orden-convergencia-aprox}) puede interpretarse
  en el siguiente sentido: puesto que $\lim x_k=\alpha$, para $k$
  suficientemente grande tendremos que el error $|x_k-\alpha|$ es
  menor que uno. Si $p>1$, el método hace decrecer el error en
  términos de esta potencia (multiplicado por la constante asintótica
  de error, $C$). Si $p=1$ debemos exigir
  $C<1$ para garantizar el descenso del error.\label{rk:1}
\end{remark}
% el que exige $C<1$ para garantizar la disminución del error)
En la mayor parte de los casos no será necesario precisar exactamente
el orden de convergencia de un método numérico y bastará determinar el
orden en el siguiente sentido:

\begin{definition}
  Diremos que un método iterativo (definido por una sucesión $\{x_k\}$ tal
  que $\lim x_k=\alpha$) tiene \resaltar{orden de convergencia al menos}
  $p\ge 1$ si existen una constante $C>0$ y un número entero $k_0>0$
  tal que
  \begin{equation}
    \label{eq:orden-convergencia-al-menos-p}
    |x_{k+1}-\alpha| \le C |x_k-\alpha|^p \quad \forall k\ge k_0.
  \end{equation}
\end{definition}

%\begin{remark}
  % La desigualdad~(\ref{eq:orden-convergencia-2}) significa que todo
  % método iterativo con orden de convergencia (exactamente) $p$ tiene
  % orden de convergencia al menos $p$ para una constante ligeramente
  % mayor, $C+\epsilon$.
Algunos comentarios:
\begin{itemize}
\item Un método iterativo de orden al menos $p$ podría tener,
  exactamente, un orden mayor que $p$. Visto de otra forma, un método
  iterativo de orden $p$ es de orden al menos $q$, para todo $1\le q <
  p$ ($C<1$ si $q=1$).
  \begin{flushright}
    \vspace{-0.75em}
    \scriptsize \em La demostración de lo anterior se deja como ejercicio.
    \vspace{-0.75em}
  \end{flushright}
\item En relación con lo anterior, aunque la sucesión $x_k$
  verifique la desigualdad~(\ref{eq:orden-convergencia-al-menos-p}),
  no tiene por qué existir el límite~(\ref{eq:orden-convergencia}). En
  concreto de~(\ref{eq:orden-convergencia-al-menos-p}) sólo podemos
  concluir que
  \begin{equation*}
    \limsup_{k\to+\infty} \frac{|x_{k+1}-\alpha|}{|x_k-\alpha|^p} < +\infty.
  \end{equation*}
  De hecho, no es difícil comprobar que la condición anterior es no
  solo necesaria, sino suficiente para que la sucesión $x_{k}$ tenga
  orden al menos $p$.
\item Análogamente a la
  observación~\ref{rk:interpretacion-orden-convergencia}, la
  desigualdad~(\ref{eq:orden-convergencia-al-menos-p}) garantiza que
  en un esquema de orden al menos $p$ tiene lugar el decrecimiento del
  error absoluto en términos de su potencia $p$ (o del factor
  $C<1$ si $p=1$).
\end{itemize}

\begin{example}
  \label{rk:2}
  La sucesión $x_k=2^{-k}$ converge a cero con orden de convergencia $p=1$.
\end{example}

El resto de este capítulo se estructura de la siguiente forma: En una
primera sección, repasamos algunos resultados teóricos para asegurar
la existencia y unicidad de solución de~(\ref{eq:raiz}). En el resto
de las secciones se estudian distintos métodos iterativos para el
cálculo de ceros de funciones de una variable, partiendo del que es,
conceptualmente, más sencillo (el método de Bisección) y llegando
hasta el método de Newton y algunas de sus variantes.  La última
sección ofrece algunas indicaciones sobre la generalización de estos
métodos para resolver sistemas de ecuaciones no lineales.

\section{Existencia y unicidad de solución. Separación de ceros}
\label{sec:tema1:exist-y-unic}

Antes abordar la resolución numérica de cualquier problema es
fundamental el realizar, en una etapa previa, un análisis del mismo
que nos permita determinar la existencia de solución. En caso
afirmativo, en la mayor parte de los casos será muy importante
asegurarnos de que esta solución es única\footnote{Por ejemplo,
  la falta de unicidad de sistemas de ecuaciones lineales está
  asociada con la singularidad de las matrices asociadas. O, en
  métodos iterativos, la no unicidad puede dar pie a
  oscilaciones espurias, es decir, a sucesiones oscilantes que no
  converjan a una solución.}.

Con frecuencia, el proceso previo para determinar los ceros de una
función consistirá en localizar intervalos en los que podamos
garantizar la existencia de una única solución (proceso que recibe el
nombre de \textit{separación de ceros o de soluciones}). Los
siguientes resultados nos proporcionan unas herramientas muy útiles
para ello.

También nos resultará de utilidad, como primera aproximación, la
representación gráfica de la función, aunque \textit{en ningún caso
  debemos confiar exclusivamente en las gráficas generadas por
  programas informáticos}: éstas deben estar respaldadas por un
análisis numérico que garantice la existencia y unicidad de solución.

\begin{theorem}[Bolzano]
  \label{thm:bolzano}
  Sea $f(x)$ una función continua en un intervalo $[a, b]$ y
  supongamos que $f (a)\cdot f (b) < 0$.  Entonces, existe
  $c\in(a, b)$ tal que $f (c) = 0$.
\end{theorem}

\begin{theorem}[Rolle]
  \label{thm:rolle}
  Sea $f(x)$ una función continua en $[a, b]$ y derivable en
  $(a, b)$ tal que $f(a) = f(b)$.
  Entonces existe al menos un valor $c \in (a, b)$ tal que $f'(c) = 0$.
\end{theorem}

% \begin{remark}[Sobre unicidad en los teoremas de Bolzano y Rolle]
%   \label{rk:tema1:unicidad-bolzano-rolle}
Estos teoremas son bien conocidos en el ámbito del análisis
matemático. El teorema de Bolzano proporciona existencia de solución,
pero unicidad (es decir, una función continua con distinto signo en
$a$ y en $b$ podría tener muchos ceros en $(a,b)$). <El teorema de
Rolle puede ser utilizado para obtener el siguiente resultado de
unicidad:

% \end{remark}

% Combinando ambos teoremas, podemos deducir el siguiente resultado de
% existencia y unicidad:

\begin{corollary}
  \label{cor:tema1:exist+unic}
  Sea $f(x)$ continua en $[a, b]$ y derivable en $(a, b)$. Si
  $f'(x)\ne 0$, para todo $x\in (a, b)$, entonces existe a lo sumo una
  solución de~\eqref{eq:raiz} en el intervalo $I=[a,b]$.
\end{corollary}

\begin{proof}
  Por reducción al absurdo, si existieran dos soluciones
  de~\eqref{eq:raiz}, $\cero_1$ y $\cero_2$, tendríamos que
  $0=f(\cero_1)=f(\cero_2)$.  Puesto que $f$ se encuentra en las
  hipótesis del teorema de Rolle en el subintervalo
  $[\cero_1,\cero_2]$ (y suponiendo por ejemplo $\cero_1<\cero_2$), esto
  implicaría que existe $c$ entre $\cero_1$ y $\cero_2$ en el que
  $f'(c)=0$, lo que contradice las hipótesis.
\end{proof}

\begin{remark}
  \label{rk:tema1:exist+unic}
  El teorema anterior puede generalizarse a intervalos
  infinito-dimensionales, de la forma $(-\infty,b]$, $[a,+\infty)$ o
  $(-\infty,+\infty)$. En efecto, en estos casos podríamos repetir la
  demostración anterior, pues es en ella es suficiente el poder
  aplicar el teorema de Rolle dentro de cualquier subintervalo
  $[\cero_1,\cero_2]$.
\end{remark}

\begin{remark}[Separación de ceros]
  \label{rk:tema1:separac-ceros}
  % algoritmo para la separación de
  % soluciones de~\eqref{eq:raiz} que será utilizado en los próximos
  % ejemplos.
  Suponiendo continuidad y derivabilidad de $f$ (para
  aplicar el Corolario~\ref{cor:tema1:exist+unic}), % suponemos que la
  % derivada, $f'$, \emph{es continua en $(a,b)$},
  podemos enunciar un algoritmo para la separación de las raíces de
  $f(x)=0$:
  \begin{enumerate}
  \item Hallar todas las raíces de $f'(x)=0$, a las que llamaremos
    $\beta_1<\dots<\beta_n$.
  \item Considerar los intervalos $(a,\beta_1)$, $(\beta_1,\beta_2)$,
    $(\beta_2,\beta_3)$, \dots,$(\beta_n,b)$. Puesto que, dentro de
    ellos, $f'(x)\neq 0$, sabemos (según el Corolario~\eqref{eq:raiz})
    que existe, a lo sumo, un cero de $f$ en cada uno de estos
    intervalos.
  \item Utilizar el teorema de Bolzano para detectar en cuales de
    estos intervalos existe realmente una raíz de $f(x)=0$.
  \end{enumerate}
  El proceso anterior es válido incluso si $a=-\infty$ o $b=+\infty$,
  según la observación~\ref{rk:tema1:exist+unic}.  Sin embargo, aunque es
  útil desde el punto de vista teórico y en algunos casos
  particulares, presenta un importante inconveniente en la práctica:
  la necesidad de determinar las raíces de  $f'$.  En
  realidad, estamos convirtiendo el problema del cálculo de ceros de
  $f$ en otro similar, el cálculo de ceros de $f'$, que podría tan
  complicado como el anterior (o más aún).
\end{remark}
Con frecuencia, se utilizan otros razonamientos para determinar
todos los intervalos, $(\beta_i,\beta_{i+1})$ en los que
$f(\beta_i)\cdot f(\beta_{i+1})<0$. Por ejemplo, procedimientos de
tipo bisección (que analizaremos en la próxima sección).

% A continuación, veremos algunos ejemplos en los que se aplican los
% resultados anteriores:

\begin{example}
  \label{ex:tema1:separ-soluc-1} 
  Nos planteamos el hallar las raíces de la ecuación
  $$
  f(x)=x^3-9x+3 =0.
  $$
  Siguiendo los pasos de la observación~\ref{rk:tema1:separac-ceros},
  calculamos la derivada de $f$,
  $$
  f'(x)=3x^2-9,
  $$
  que sólo se anula en $x=-\sqrt 3,
  x=+\sqrt 3$. Por lo tanto existen, a lo sumo, tres ceros de $f$ que
  están localizados en
  $$
  (-\infty,-\sqrt 3), (-\sqrt 3, +\sqrt 3) \text{ y } (+\sqrt 3,
  +\infty).
  $$

  Solamente resta utilizar el teorema de Bolzano para detectar en
  cuáles de estos intervalos existe una raíz de
  $f(x)=0$. Como apoyo, podemos utilizar un entorno informático
  para representar la gráfica (figura~\ref{fig:tema1:ejemplo-separ-soluc-1}),
  que en este caso nos sugiere que, concretamente, las raíces pueden
  ser localizadas en los intervalos $[-4,-3]\subset (-\infty,-\sqrt
  3)$, $[0,1]\subset (-\sqrt 3, +\sqrt 3)$ y $[2,3]\subset (+\sqrt 3,
  +\infty)$.
  \begin{figure}
    \label{fig:tema1:ejemplo-separ-soluc-1}
    \begin{graficaTikz}[width=23em, height=15em]
      \begin{axis}[\axisXYmiddle]
        % Draw a curve
        \addplot[domain=-4.0:4.0, blue, ultra thick, samples=40]
        {x^3-9*x+3};
        % Plot a label at curve root
        \node[coordinate, medium dot, pin=95:{$\cero_1$}] at 
        (axis cs:-3.15,0) {};
        \node[coordinate, medium dot, pin=85:{$\cero_2$}] at 
        (axis cs:0.33,0) {};
        \node[coordinate, medium dot, pin=95:{ $\cero_3$}] at 
        (axis cs:2.81,0) {};
      \end{axis}
    \end{graficaTikz}
    \caption{Gráfica de $f(x)=x^3-9x+3$}
  \end{figure}
  En efecto, utilizando el teorema de Bolzano:
  \begin{itemize}
  \item En $[-4,-3]$: $f(-4)=-25$ y $f(-3)=3$, por lo tanto existe (al
    menos) un valor   $\cero_1 \in (-4,3)$ tal que $f(\cero_1)=0$.
  \item En $[0,1]$: $f(0)=3$ y $f(1)=-5$, luego existe
    $\cero_2 \in (0,1)$ tal que $f(\cero_2)=0$.
  \item En $[2,3]$: $f(2)=-7$ y $f(3)=3$, por tanto existe
    $\cero_3 \in (2,3)$ tal que $f(\cero_3)=0$.
  \end{itemize}
\end{example}

\begin{example}
  Determinaremos el número de valores $x\in\Rset$ tales que
  $2x=cos(x)$ y localizaremos estos valores en intervalos (lo que, en
  las próximas secciones, se usará para aplicar métodos numéricos para
  aproximar las soluciones).

  Para ello, planteamos el problema~\eqref{eq:raiz} para la función
  continua y derivable
  $$
  f(x)=2x-\cos(x)
  $$ 
  y, siguiendo las ideas de la
  observación~\ref{rk:tema1:separac-ceros}, comenzamos estudiando la
  derivada
  $$
  f'(x)=2+\sen(x).
  $$
  Puesto que $|\sen(x)|\le 1$, tenemos $f'(x)\neq 0$ para todo
  $x\in\Rset$, luego existe, como máximo, una raíz de $f(x)=0$ en
  $(-\infty,+\infty)$.

  \begin{figure}
    \label{fig:tema1:ejemplo-separ-soluc-2}
    \begin{graficaTikz}[width=23em, height=15em]
      \begin{axis}[\axisXYmiddle] 
        % Draw a curve
        \addplot[domain=-pi:pi+0.3, blue, ultra thick, samples=40]
        {{2*x-cos(deg(x))}}; 
        % Plot a label at curve root 
        \node[coordinate, medium dot, pin=-87:{$\cero$}] at (axis cs:0.45,0) {};
      \end{axis}
    \end{graficaTikz}
    \caption{Gráfica de $f(x)=2x-\cos(x)$}
  \end{figure}
  La gráfica de $f$ (figura~\ref{fig:tema1:ejemplo-separ-soluc-2}) nos
  sugiere que esta raíz, $\alpha$, es positiva. Por ejemplo si
  aplicamos el teorema de bolzano en el intervalo $[0,\pi/2]$ (elegido
  por resultar sencilla la evaluación de $f$) tenemos: $f(0)=-1$ y
  $f(\pi/2)=\pi$.  Así, $x=\alpha$ es el único valor, concretamente
  localizado en $(0,\pi/2)$, tal que $2x=\cos(x)$.
\end{example}

\section{El método de bisección}
\label{sec:tema1:bisecc}

Comenzamos presentando el método más sencillo, basado en el teorema de
Bolzano. Para ello, supongamos que $f$ es una función continua en
$[a,b]$ tal que $f(a)f(b)<0$, por tanto existe algún cero
de $f$ en $(a,b)$, al que llamaremos $\alpha$.

Por simplicidad, supondremos que $\alpha$ es el único cero de $f$ en
$(a,b)$, en otro caso podríamos separar los ceros en subintervalos,
véase la sección~\ref{sec:tema1:exist-y-unic}. Aunque veremos que este
método sigue siendo válido aunque no haya unicidad de solución
(observación~\ref{rk:tema1:unicidad-bisecc}).

La idea del método de bisección es dividir por la mitad el intervalo y
considerar los dos subintervalos resultantes, para seleccionar aquél
en el que $f$ cambia de signo (y que contiene a la raíz, según el
teorema de Bolzano). Con más detalle: el método consiste en construir
una sucesión de intervalos,
\begin{equation}
\label{eq:tema1:bisecc:0}
[a,b]=[a_0,b_0] \supset [a_1,b_1] \supset [a_2,b_2] \cdots \supset
[a_n,b_n] \supset \cdots
\end{equation}
definida a partir de sus puntos medios, $c_n=(a_n+b_n)/2$, tal y como sigue:
\begin{itemize}
\item Como inicialización, tomamos $a_0=a$ y $b_0=b$ y calculamos
  $c_0=(a_0+b_0)/2$.
\item En cada etapa $k\ge 1$, construimos un intervalo $[a_k,b_k]$ a
  partir del anterior, $[a_{k-1}, b_{k-1}]$, de la siguiente forma:
  \begin{enumerate}
  \item Calculamos el punto medio del intervalo anterior,
    $c_{k-1}=(a_{k-1}+b_{k-1})/2$. Si $f(c_{k-1})=0$ hemos terminado
    ($c_{k-1}$ es el cero de $f$).
  \item En otro caso definimos $a_k$ y $b_k$ de forma que $f$ cambie de
    signo en $[a_k,b_k]$:
    \begin{enumerate}
    \item Si $f(a_{k-1})f(c_{k-1})<0$, elegimos $[a_k,b_k]=[a_{k-1}, c_{k-1}]$.
    \item Si $f(c_{k-1})f(b_{k-1})<0$, elegimos $[a_k,b_k]=[c_{k-1}, b_{k-1}]$.
    \end{enumerate}
  \item A continuación, pasamos a la siguiente etapa (incrementamos
    $k$).
  \end{enumerate}
\end{itemize}

En la práctica, es muy improbable que $f(c_k)=0$ y deben establecerse
condiciones para evitar bucles infinitos. Puede verse una
implementación del método de bisección en forma de
pseudocódigo%
\footnote{En realidad, el pseudocódigo que se utilizará en
  este manual constituirá código conforme a Python 2.x} en el
Algoritmo~\ref{alg:metodo-biseccion}.
\begin{algorithm}  \begin{python}
def biseccion(f, a, b, tol, max_iters):
  iter = 0
  while b-a >= tol and iter < max_iters:
      c = (a+b)/2.0
      if f(c)==0:   
          return c
      else:
          if f(a)*f(c) < 0:
              b=c
          else:
              a=c
      iter = iter + 1
  return c    
\end{python}
\caption{Método de bisección}
\label{alg:metodo-biseccion}
\end{algorithm}

\begin{example}
  Aplicaremos el método de bisección a la función $f(x)=x^3-9x+3$ en el intervalo
  $[0,1]$, donde sabemos que existe un único cero (según vimos en el
  ejemplo~\ref{ex:tema1:separ-soluc-1}). El método consiste en los
  siguientes pasos (resumidos en el cuadro~\ref{tab:tema1:bisecc}):

  \begin{itemize}
  \item Empezamos seleccionando $[a_0,b_0]=[0,1]$.
  \item En la primera etapa, $k=1$, calculamos 
    $c_0=(1-0)/2=0.5$  y 
    $f(c_0)=f(1/2)=-11/8=-1.375$. Como $f(a_0)=f(0)=3$, elegimos
    $[a_1,b_1]=[a_0,c_0]=[0,0.5]$.
  \item Repetimos el proceso para $k=2,3,...$, obteniendo los resultados que se
    muestran en el cuadro~\ref{tab:tema1:bisecc}.
  \end{itemize}

\end{example}
\begin{table}
  \centering
  \rule{0.99\linewidth}{1.6pt}
  \begin{equation*}
    \begin{array}{l<{\quad}rrrrr}%{>{$}r<{$}>{$}r<{$}>{$}r<{$}>{$}r<{$}>{$}r<{$}}    
     k &  [a_k,b_k] & c_k & f(a_k) & f(b_k) & f(c_k) 
      \\ \toprule%\mbox{}% \mbox{} for avoiding bug with "["
     0 & [0,1]  &  0.5 
      & 3 & -5 & -1.375
      \\ \noalign{\smallskip}
     1 &  [0,0.5] &  0.25
      & 3 & -1.375 & 0.76
      \\ \smallskip
     2 & [0.25,0.5] & 0.375
      & 0.76 & -1.375 & -0.32
     \\ \smallskip
     3& [0.25,0.375] & 0.3125
      & 0.76 & -0.32 & 0.21
      \\
      \hfill \vdots \hfill~ & \hfill \vdots \hfill~ & 
      \hfill \vdots \hfill~ & \hfill \vdots \hfill~ & \hfill \vdots \hfill~
    \end{array}
  \end{equation*}
  \rule{0.99\linewidth}{1.5pt}
  \caption{Método de bisección para $f(x)=x^3-9x-3$ en $[0,1]$.}
  \label{tab:tema1:bisecc}
\end{table}

Como se puede observar en el ejemplo anterior, el método de bisección
genera una sucesión de intervalos $[a_k,b_k]$ que
contienen la solución de~\eqref{eq:raiz} (pues, por construcción,
$f(a_k)f(b_k)<0$). Además, el tamaño $b_k-a_k$ converge a cero (puesto
que, en cada paso, el tamaño se divide por dos), de hecho:
\begin{equation}
  \label{eq:tema1:bisec:1}
  b_k-a_k = \frac{b-a}{2^k} \quad \forall k=0,1,2,...
\end{equation}

Así, se tiene el siguiente resultado:
\begin{theorem}
  \label{thm:tema1:bisecc}
  Sea $f$ una función continua en $[a,b]$ tal que $f(a)f(b)<0$.
  \begin{enumerate}
  \item La sucesión $\{c_k\}_{k=0}^\infty$ (definida por los puntos
    medios de los intervalos en el método de bisección) es convergente
    y su límite, $\cero$, es un cero de $f$ en $(a,b)$.
  \item De hecho, se verifica la siguiente cota del error:
    \begin{equation}
      \label{eq:tema1:bisecc:2}
      |c_k-\cero| \le \frac{b-a}{2^k}.
    \end{equation}
  \end{enumerate}
\end{theorem}

\begin{proof}
  Los intervalos $[a_k,b_k]$ generados por el método de bisección
  verifican~\eqref{eq:tema1:bisecc:0} y $f(a_k)f(b_k)<0$, por lo tanto
  todos ellos contienen un cero, $\cero\in (a,b)$. Además, como
  $\cero\in [a_k,b_k]$ (y también $c_k$),
  $$
  |c_k - \cero| < b_k - a_k.
  $$
  Lo anterior junto a~\eqref{eq:tema1:bisec:1},
  implica~\eqref{eq:tema1:bisecc:2}. Y, a su vez,
  \eqref{eq:tema1:bisecc:2} implica que $c_k$ converge a $\cero$.
\end{proof}

\begin{remark}[Ventajas e inconvenientes del método de bisección]
  \label{rk:tema1:unicidad-bisecc}
  El teorema~\ref{thm:tema1:bisecc} es válido incluso si $f$ tiene
  varios ceros en $[a,b]$, es decir el método de bisección tiene la
  importante propiedad de que siempre converge hacia una solución,
  independientemente de que ésta sea única, y sin más hipótesis sobre
  $f$ que su continuidad.

Sin embargo, tiene el inconveniente de que su velocidad de
convergencia es lenta (no alcanzando siquiera orden $1$), necesitando
muchas más iteraciones que otros métodos que veremos más tarde. De
hecho, el método no garantiza que el error se reduzca en cada
iteración: solamente que el intervalo se reduce a la mitad. Véase que,
el método no utiliza ningún tipo de información acerca de la función
$f$ (salvo su signo) y de hecho no podemos esperar convergencia en una
sola iteración, ni siquiera en el caso en el que $f$ sea lineal.

De cualquier forma, el algoritmo de bisección es un método sencillo y
muy útil como ``método de arranque'', con el que realizar iteraciones
previas antes de pasar a otros métodos más eficientes.
 \end{remark}

\begin{remark}
  \label{rk:tema1:bisecc:iteraciones}
  La desigualdad~\eqref{eq:tema1:bisecc:2} nos garantiza que, para $n$
  suficiente grande, podemos aproximar un cero de $f$ con un error tan
  pequeño como deseemos. Y, de hecho, dada una tolerancia $\varepsilon$,
  para que $|c_k-\alpha|<\varepsilon$ podemos calcular el mínimo número
  de iteraciones tal que que $(b-a)/2^k < \varepsilon$. Efectivamente,
  despejando $n$ en~\eqref{eq:tema1:bisecc:2}, se trata del menor
  entero positivo $n_{\text{mín}}$ tal que:
  \begin{equation*}
    n_{\text{mín}}>\log_2\left(\frac{b-a}{\varepsilon}\right).
  \end{equation*}
\end{remark}


\begin{test}[Función ``bisección'']
  El programa~\ref{pro:metodo-biseccion} muestra una implementación
  del método de bisección, utilizando el lenguaje de programación
  \textit{Python}, en la que se mejora el
  Algoritmo~\ref{alg:metodo-biseccion} (minimizando el número de
  llamadas a $f$). Concretamente, la función
  \pythoninline{biseccion(f, a, b)} calcula una solución
  de~\eqref{eq:raiz} en un intervalo $(a,b)$. A través de parámetros
  opcionales se puede determinar una tolerancia, $\varepsilon$, de
  forma que el ciclo de iteraciones de detenga cuando
  $b_k-a_k<\varepsilon$. La igualdad~(\ref{eq:tema1:bisec:1})
  garantiza que así sera para $k$ suficientemente grande (dado
  concretamente en la
  observación~\ref{rk:tema1:bisecc:iteraciones}). Adicionalmente, la
  función permite fijar el número máximo de iteraciones permitidas y
  seleccionar si se actuará de forma silenciosa o bien con
  verbosidad al mostrar los resultados.

  Por ejemplo, el siguiente código \textit{Python} aproxima un cero de
  la función $f(x)$ introducida en el
  ejemplo~\ref{ex:tema1:separ-soluc-1} 
  con tolerancia $10^{-2}$:
  \pythonexternal{tema1/src/biseccion-test1.py}
  \begin{pythonoutput}
    \pythonexternal[backgroundcolor=\color{white},title={Resultado}]{tema1/src/biseccion-test1.out}
  \end{pythonoutput}
\end{test}


\begin{program}
  \widepythonexternal{tema1/src/biseccion.py}
  \label{pro:metodo-biseccion}
  \caption{Una implementación del método de bisección}
\end{program}


\section{Los métodos de punto fijo}
\label{sec:metodos-de-punto-fijo}

En este capítulo introducimos un tipo de métodos que son definidos a
partir de una reformulación de~(\ref{eq:raiz}). Se trata de los
métodos de punto fijo o de aproximaciones sucesivas.
Llamamos \resaltar{punto fijo} de una función continua
$g:I\subset\Rset\to\Rset$ a una solución del siguiente  problema:
\begin{equation}
\tag{$P_{\text{PF}}$}
\text{Hallar $\cero\in I$ tal que} \quad \cero=g(\cero).
\label{eq:punto-fijo}
\end{equation}

Geométricamente, una solución de la ecuación $x=g(x)$ se corresponde
con la abscisa correspondiente a un punto de corte entre la gráfica de
la función $y=g(x)$ y la recta $y=x$ (ver
figura~\ref{fig:ejemplo-punto-fijo-1}).

\begin{example}
  La función $g(x)=2-x^2$ tiene puntos dos puntos fijos en $x=1$ y
  $x=-2$, pues $g(1)=2-1=1$ y $g(-2)=2-4=-2$. No tiene más puntos
  fijos en $\Rset$, pues éstos son raíces de $g(x)-x$, que en este
  caso es un polinomio de grado dos. En la
  figura~\ref{fig:ejemplo-punto-fijo-1} se representan geométricamente
  esta función y sus dos puntos fijos.
\end{example}

\begin{figure}
  \begin{graficaTikz}[width=18em, height=15em]
    \begin{axis}[\axisXYmiddle, 
      % legend style = {anchor=north west, pos = north east}]
      legend pos = outer north east, legend cell align=left]
      % Draw a curve
      \addplot[domain=-2.4:2.4, blue, ultra thick, samples=40] {2-x^2};
      \addplot[domain=-2.5:2.5, gray, ultra thick, samples=40] {x};
      % Plot a label at curve root
      \node[coordinate, medium dot, pin=0:{\scriptsize$(1,1)$}] 
      at (axis cs:1,1) {};
      \node[coordinate, medium dot, pin=-45:{\scriptsize$(-2,-2)$}] 
      at (axis cs:-2,-2) {};
      \legend {$y=g(x)$,$y=x$};
    \end{axis}
  \end{graficaTikz}
  \caption{Puntos fijos de $g(x)=2-x^2$}
  \label{fig:ejemplo-punto-fijo-1}
\end{figure}

Si $\cero$ es una solución de un problema de punto fijo, entonces
$\cero$ es solución de un problema de cálculo de raíces del
tipo~(\ref{eq:raiz}) para $f(x)=x-g(x)$. Recíprocamente un problema de
cálculo de raíces puede ser transformado en un problema del
tipo~(\ref{eq:punto-fijo}) de numerosas formas, por ejemplo
escribiendo $g(x)=x-f(x)$ o empleando otro tipo de manipulaciones
algebraicas (véase el ejemplo~\ref{ex:punto-fijo-1}). Algunas de estas
transformaciones en ecuaciones de tipo punto fijo pueden dar lugar a
potentes técnicas iterativas.

A continuación, estudiamos resultados de existencia y unicidad de
punto fijo.

\begin{proposition}[Existencia de solución de~(\ref{eq:punto-fijo})]
  \label{pro:existencia-punto-fijo}
  Sea $g:[a,b]\to\Rset$ una función continua en $[a,b]$ y supongamos
  que
  \begin{equation}
    g([a,b])\subset [a,b].
  \label{eq:g[a,b].subset.[a,b]}  
\end{equation}
  Entonces existe al menos un punto fijo de $g$ en $[a,b]$.
\end{proposition}
\begin{proof}
  Debido a la hipótesis~(\ref{eq:g[a,b].subset.[a,b]}),
  \begin{extension}
    La hipótesis~(\ref{eq:g[a,b].subset.[a,b]}) se puede debilitar, en
    concreto es suficiente que $g(x)$ verifique
    $$(g(a)-a)(g(b)-b)\le 0$$.
  \end{extension}
  $g(a)\ge a$ y
  $g(b)\le b$, por lo tanto podemos aplicar el teorema de Bolzano
  (teorema~\ref{thm:bolzano}) a la función $f(x)=x-g(x)$ en $[a,b]$.
\end{proof}

\subsection*{Funciones contractivas}

Para estudiar la unicidad de solución de~(\ref{eq:punto-fijo}),
introduciremos la siguiente definición:

\begin{definition}
  Una función $g$ continua en $[a,b]$ se dice \resaltar{contractiva} en
  $[a,b]$ si existe $\cteContract\in[0,1)$ tal que
  \begin{equation*}
    |g(x)-g(y)| \le \cteContract |x-y|, \quad \forall x,y \in [a,b].
    \label{eq:contractividad}
  \end{equation*}
  A $\cteContract$ se le llama constante de contractividad de $g$ en $[a,b]$.
  \label{def:funcion.contractiva}
\end{definition}

\begin{example}
La función $g(x)=(x^2-1)/3$ es contractiva en $[-1,1]$, pues
$$
|g(x)-g(y)|=\left|\frac{x^2-y^2}{3}\right| = \frac{|x+y|}{3}|x-y| \le
\cteContract |x-y|,
$$
para $\cteContract=\max \big\{ \frac{|x+y|}{3}\ /\ x,y\in [-1,1]
\big\} =\frac23 <1$.
\end{example}

\begin{remark}[Algunas propiedades de las funciones contractivas]~
  \begin{itemize}
  \item Puesto que $\cteContract<1$, toda función contractiva, $g$, <<contrae las
    distancias>> en el sentido de que $|g(x)-g(y)|<|x-y|$ para todo
    $x,y\in [a,b]$.
  \item Toda función contractiva en $[a,b]$ es
    uniformemente continua en ese mismo intervalo (es decir, para
    todo $\varepsilon>0$ existe $\delta>0$ tal que $|x-y|<\delta
    \Rightarrow |f(x)-f(y)|<\varepsilon$).
\end{itemize}
\end{remark}

\begin{proposition}[Condición suficiente de contractividad]
  \label{pro:1}
  Supongamos que $g\in C([a,b])$ y derivable en $(a,b)$, tal que
  \begin{equation}
    \cteContract=\sup_{x\in(a,b)} |g'(x)|<1.
    \label{eq:L=sup|g'|<1}
  \end{equation}
  Entonces $g$ es contractiva en $[a,b]$ y $\cteContract$ es una
  constante de contractividad para $g$.
\end{proposition}
\begin{proof}
  Sean $x,y\in [a,b]$, por ejemplo $x<y$. Aplicando el teorema del
  valor medio en $[x,y]$, deducimos que existe $c\in (x,y)$ tal que
  \begin{equation*}
    |g(x)-g(y)|=|g'(c)(x-y)| \le \cteContract |x-y|,
  \end{equation*}
  donde $\cteContract$ viene dada por~(\ref{eq:L=sup|g'|<1}).
\end{proof}

\begin{remark}
  Si $g'$ es continua en $[a,b]$, es suficiente
  para~(\ref{eq:L=sup|g'|<1}) que $g'(x)<1$ para todo $x\in[a,b]$. 
  La demostración: $\sup_{x\in(a,b)}|g'(x)| \le \sup_{x\in[a,b]}
  |g'(x)|$ y, debido al teorema de Weierstrass, $g'$ alcanza su máximo
  en algún punto $c\in [a,b]$.
  \label{rk:3}
\end{remark}
Es sencillo demostrar que las funciones contractivas no pueden tener
más de un punto fijo:

\begin{proposition}[Unicidad de solución de~(\ref{eq:punto-fijo})]
  \label{pro:unicidad-punto-fijo}
  Sea $g:[a,b]\to\Rset$ una función \emph{contractiva} en
  $[a,b]$. Entonces $g$ posee, a lo sumo, un punto fijo en $[a,b]$.
\end{proposition}

\begin{proof}
  Si suponemos que $g$ tiene dos puntos fijos, $\cero_1$ y
  $\cero_2$, llegamos inmediatamente a una contradicción:
  $$
  |\cero_1-\cero_2| = |g(\cero_1)-g(\cero_2)| \le \cteContract |\cero_1 -
  \cero_2| < |\cero_1-\cero_2|,$$
  donde $\cteContract<1$ es la constante de contractividad de $g$.
\end{proof}

\subsection*{Métodos de punto fijo}

Los problemas de punto fijo~(\ref{eq:punto-fijo}) dan lugar al
siguiente tipo de esquemas recursivos, conocidos como métodos de
aproximaciones sucesivas (o simplemente, \resaltar{métodos de punto
  fijo}):
\begin{equation}
  \tag{$M_{\text{AS}}$}
  \left\{
    \begin{array}{l}
      \text{Dado } x_0\in [a,b], \\
      \text{calcular } x_{k+1}=g(x_k), \quad \forall k\ge 0.
    \end{array}
    \right.
  \label{eq:MAS}
\end{equation}
Véase que para que el método esté \textit{bien definido} es necesario
que $x_k$ se encuentre en el dominio de $g$ para todo $k$.
El siguiente teorema resume los resultados de existencia y unicidad
enunciados en las Proposiciones~\ref{pro:existencia-punto-fijo}
y~\ref{pro:unicidad-punto-fijo}, a la vez que garantiza el buen
planteamiento y la convergencia de~(\ref{eq:MAS}) bajo las hipótesis
de aquellas proposiciones.

\begin{theorem}[Teorema del punto fijo]
  \label{thm:punto-fijo-Banach}
  Sea $g:[a,b]\to\Rset$ tal que $g([a,b]) \subset [a,b]$ y supongamos
  que $g$ es contractiva en $[a,b]$ con constante de contractividad
  $\cteContract\in [0,1)$. Entonces:
  \begin{enumerate}
  \item 
    \label{item:punto-fijo-Banach:1}
    La función $g$ tiene un \textsf{único punto fijo}, $\cero$, en
    $[a,b]$.
  \item 
    \label{item:punto-fijo-Banach:2}
    Para todo $x_0\in [a,b]$, el método~(\ref{eq:MAS}) está bien
    definido y es \textsf{convergente} hacia $\alpha$.
  \item En cada etapa de~(\ref{eq:MAS}) se tienen las siguientes
    \textsf{estimaciones} del error absoluto:
    \label{item:punto-fijo-Banach:3}
    \begin{align}
      \label{eq:pto-fijo:cota-a-priori}
      |x_k-\cero| \le \cteContract^k &|x_0-\cero|,
      \\
      \label{eq:pto-fijo:cota-a-posteriori}
      |x_k-\cero| \le \frac{\cteContract^k}{1-\cteContract}
      &|x_1-x_0|.
    \end{align}
  \end{enumerate}
\end{theorem}

La acotación~(\ref{eq:pto-fijo:cota-a-priori}) llama
\textit{estimación a priori}, mientras que
a~(\ref{eq:pto-fijo:cota-a-posteriori}) se la conoce como
\textit{estimación a posteriori} del error.

\begin{proof}~\par
  El punto~\ref{item:punto-fijo-Banach:1} no es más que una
  reiteración de las Proposiciones~\ref{pro:existencia-punto-fijo}
  y~\ref{pro:unicidad-punto-fijo}.  Respecto al
  punto~\ref{item:punto-fijo-Banach:2}, para cualquier $x_0\in [a,b]$,
  el método iterativo está bien definido, ya que para $k>1$,
  $x_{k}=g(x_{k-1})$ y $g([a,b])\subset [a,b]$.
  Acerca de la convergencia, dado $x_0\in
  [a,b]$ podemos aplicar repetidamente la función $g$ obteniendo:
  \begin{equation}
  \begin{aligned}
    |x_k-\cero| = &|g(x_{k-1})-g(\cero)| \le \cteContract
    |x_{k-1}-\cero| = \\
    = \cteContract &|g(x_{k-2})-g(\cero)| \le \cteContract^2 
    |x_{k-2}-\cero| \le \cdots \le \cteContract^k |x_0-\cero|.
  \end{aligned}\label{eq:1}  
\end{equation}
  Así se tiene la estimación a
  priori~(\ref{eq:pto-fijo:cota-a-priori}) y, como
  $L<1$, podemos concluir que $x_k\to\cero$.

  Para demostrar la estimación~(\ref{eq:pto-fijo:cota-a-posteriori})
  comenzamos con un razonamiento análogo al anterior. Utilizando que
  $g$ es contractiva (y que $x_k\in [a,b]$ para todo $k\ge 0$) tenemos
  que para todo $n \ge 0$:
  \begin{equation*}
    |x_{n+1}-x_{n}| =
    |g(x_{n})-g(x_{n-1})|\le\cteContract|x_{n}-x_{n-1}| \le \cdots \le \cteContract^n|x_1-x_0|.
  \end{equation*}
  Sean ahora $k,n\in\Nset$ y supongamos $n>k$. Aplicando la
  desigualdad triangular junto a la desigualdad anterior obtenemos:
  \begin{align*}
     |x_n-x_k| &\le |x_n-x_{n-1}| + |x_{n-1}-x_{n-2}| + \cdots +
     |x_{k+2}-x_{k+1}| + |x_{k+1}-x_{k}|
     \\
     &\le \left(\cteContract^{n-1} + \cteContract^{n-2} +\cdots+
       \cteContract^{k+1} + \cteContract^{k} \right) |x_1-x_0|
     \\
     &=\left(\frac{\cteContract^k-\cteContract^n}{1-\cteContract}\right)
     |x_1-x_0| 
     \le \frac{L^k}{1-L} |x_1-x_0|.
  \end{align*}
  Y por último, usando que $x_n\to\cero$,
  podemos llegar a~(\ref{eq:pto-fijo:cota-a-posteriori}).
\end{proof}

\begin{algorithm}  \begin{python}
def punto_fijo(g, x0, tol, max_iters):
    iter = 0
    while iter < max_iters:
        x1 = g(x0)
        if abs(x1-x0) < tol: 
            return x1
        else:
            x0 = x1
            iter = iter + 1        
    print "Fallo de convergencia en el método de punto fijo!"
\end{python}
\caption{Método de punto fijo (o aproximaciones sucesivas)}
\label{alg:metodo-punto_fijo}
\end{algorithm}
El método de punto fijo o de aproximaciones sucesivas se ha enunciado
en el algoritmo~\ref{alg:metodo-punto_fijo}.  Dada una tolerancia
$\epsilon$, el algoritmo realiza iteraciones hasta que se verifica el
test de parada $|x_{k+1}-x_k|<\epsilon$. Si $g$ es coerciva, este test
de parada está bien formulado, pues implica $|\cero-x_k|<\epsilon$
(ejercicio: demostrar lo anterior, usando la desigualdad triangular y
la coercividad de $g$).% En efecto, usando la
% desigualdad triangular,
% \begin{align*}
%   |\cero-x_k| \le |\cero-x_{k+1}|+|x_{k+1}-x_k|
%   = |g(\cero)-g(x_k)| + |x_{k+1}-x_k|
%   \le \lambda|\cero-x_k| + |x_{k+1}-x_k|
% \end{align*}

\begin{remark}[Orden uno de~(\ref{eq:MAS})]
   En las hipótesis del Teorema~\ref{thm:punto-fijo-Banach} los
    métodos de punto fijo tienen \resaltar{orden de convergencia al
      menos uno}.  En efecto, si $g$ verifica estas hipótesis,
    siguiendo la demostración anterior tenemos la cadena de)
    desigualdades~(\ref{eq:1}). Si nos fijamos en la primera
    desigualdad,
    $$
    |x_k-\cero|\le\cteContract |x_{k-1}-\cero|,
    $$
    por tanto se verifica la condición
    (\ref{eq:orden-convergencia-al-menos-p}) para orden $p=1$ (con
    constante $C=\lambda<1$).
  \end{remark}    
  \begin{remark}[Orden $p$ de~(\ref{eq:MAS})]
    Para hipótesis más restrictivas sobre $g$ se puede llegar a
    \resaltar{orden mayor que uno}. En concreto, se puede demostrar
    que si~(\ref{eq:MAS}) converge a un punto fijo, $\cero$, y si
    $g\in C^p([a,b])$ siendo $p \ge 1$ un entero, con
    \begin{extension}
      (En el caso $p=1$ es necesario exigir $g'(\cero)<1$).
    \end{extension}
    \begin{equation*}
      g'(\cero)=g''(\cero)=\cdots=g^{p-1)}(\cero)=0, \quad
      g^p)(\cero)\neq 0,
    \end{equation*}
    entonces~(\ref{eq:MAS}) tiene orden exactamente $p$.
    \begin{extension}
      (Siempre que no ocurra que $x_k=\alpha\ \forall k\ge k_0$).
    \end{extension}
    La demostración se basa en un desarrollo de Taylor de $g$ hasta
    orden $p$.
    \label{rk:MAS.orden.p}
  \end{remark}
%\subsection{Ejemplos}

\begin{example}
  Nos proponemos el formular en términos de problemas de punto fijo el
  cálculo de todos los ceros de $f(x)=\sen(x)-4x^2+1$. 

  
  \textbf{Primera etapa:} Determinar cuántos ceros son y en qué
  intervalos se encuentran. Como $f$ es continua y derivable en todo
  $\Rset$, utilizaremos el algoritmo de separación de ceros enunciado
  en la observación~\ref{rk:tema1:separac-ceros}.
  \begin{enumerate}
  \item Debemos primero localizar los ceros de la primera
    derivada, planteando $f'(x)=\cos(x)-8x=0$, problema cuya solución
    no es inmediata.
  \item Aun así, aprovecharemos que $f'(x)$ es a su vez continua y
    derivable, con $f''(x)=-\sin(x)-8$. Como $f''(x)<0$ para todo
    $x\in\Rset$, el corolario~\ref{cor:tema1:exist+unic} (aplicado a
    $f'$) implica que $f'$ tiene un único cero en $\Rset$.
  \item Usando de nuevo el
    corolario~\ref{cor:tema1:exist+unic}, aplicado en esta ocasión a
    $f$, concluimos que $f$ tiene exactamente dos ceros en $\Rset$
    (separados por el cero de $f'$).
  \item Por último, el teorema de Bolzano nos permitirá determinar dos
    intervalos $[a_1,b_1]$ y $[a_2,b_2]$ que contengan estos dos
    ceros. Podemos proceder al análisis de las regiones donde $f$ es
    positiva y negativa, o bien utilizar un programa de ordenador para
    representar la gráfica como ayuda. En cualquier caso, tendremos
    que probar con distintos intervalos hasta encontrar aquellos en
    los que se verifique el Teorema de Bolzano (y más adelante, las
    hipótesis del Teorema de punto fijo). En este caso, una primera
    elección, válida para el Teorema de Bolzano es:
    \begin{itemize}
    \item $[a_1,b_1]=[-\pi/2, 0]$, pues $f(-\pi/2)=-1-\pi^2+1<0$ y
      $f(0)=1>0$,
    \item $[a_2,b_2]=[0, \pi/2]$, pues $f(0)=1>0$ y $f(\pi/2)= 1-\pi^2+1<0$
    \end{itemize}
    El problema es que la función de punto fijo $g_1$ que definiremos
    más abajo no es derivable en $x=-\pi/2$. Por ello, restringiremos
    este intervalo, definiéndolo tal y como sigue:
    \begin{itemize}
    \item $[a_1,b_1]=[-\pi/4, 0]$. Seguimos estando en las hipótesis
      del Teorema de Bolzano, pues $f(-\pi/4)=
      -\sqrt{2}/2-\pi^2/4+1=-2.174...<0$ y $f(0)=1>0$,
    \end{itemize}
  \end{enumerate}

  \textbf{Segunda etapa}: intentamos reescribir, en cada uno de
  estos intervalos, el problema en términos de un esquema de punto
  fijo que esté en las hipótesis del teorema~\ref{thm:punto-fijo-Banach}. La
  posibilidad más sencilla a partir de $\sen(x)-4x^2+1=0$ es despejar
  $x=\pm \frac 12 \sqrt{1+\sen(x)}$.
  \begin{itemize}
  \item En $[a_1,b_1]=[-\pi/4, 0]$, como $x<0$, definiremos el
    problema de punto fijo
  $$
  x=g_1(x)=-\dfrac 12 \sqrt{1+\sen(x)}.
  $$
  La función $g_1$ está bien definida (pues $1+\sen(x)\ge 0$) y es
  continua en todo $\Rset$. Veamos que está en las hipótesis del
  teorema de punto fijo. En primer lugar,
  $$
  g_1'(x)=\frac{-\cos(x)}{4\sqrt{1+\sen(x)}},
  $$
  que está bien definida para todo $x\in[-\pi/4, 0]$, ya que
  $\sen(x)>-1$ en este intervalo. Además:
  \begin{enumerate}
  \item $g_1([a_1,b_1])\subset [a_1,b_1]$: En efecto, $g_1$ es
    decreciente en $[-\pi/4,0]$ (pues $\cos(x)\ge 0$ si $x\in
    [-\pi/4, 0]$ y por tanto $g_1'(x)\le 0$ en este intervalo). 
    Como
    \begin{align*}
      g_1(-\pi/4)&= -\frac12 \sqrt{1+\sen(-\pi/4)}=-0.270...\in
      [-\pi/4,0]=[-0.785...,0],
      \\
      g_1(0)&=-1/2 \in [-\pi/4,0], 
    \end{align*}
    se tiene que $g([-\pi/4,0])\subset
    [g(0),g(-\pi/4)] \subset [-\pi/4,0]$.
  \item Veamos que $g_1$ es contractiva en $[a_1,b_1]$, para lo que es
    suficiente que $|g'(x)|<1$ en $[a_1,b_1]=[-\pi/4,0]$ (según la
    proposición~\ref{pro:1} y a la observación~\ref{rk:3}).  Como
    $\sen(x)$ es creciente en $[-\pi/4,0]$, en este intervalo se
    verifica que
    $$4\sqrt{1+\sen(x)}\ge 4\sqrt{1+\sen(-\pi/4)}=2.164... >1,$$ 
    así
    $$
    \left|g'(x)\right| = \frac{|\cos{x}|}{4\sqrt{1+\sen(x)}} \le
    \frac{1}{4\sqrt{\sen(x)+1}} < 1.
    $$
  \end{enumerate}
\item En $[a_2,b_2]=[0,\pi/2]$ podemos proceder de forma similar para
  $g_2(x)=+\frac 12 \sqrt{1+\sen(x)}$, obteniendo que
  $g_2([0,\pi/2])\subset[0,\pi/2]$ y $g_2$ es contractiva, por lo que
  $g_2$ está en las hipótesis del Teorema del punto fijo en
  $[a_2,b_2]$. El desarrollo de este apartado se deja como ejercicio
  al lector.
  \end{itemize}
  La \textbf{tercera y última etapa} consistiría en
  aplicar~(\ref{eq:MAS}) para aproximar las soluciones en cada uno de
  los dos intervalos. Por ejemplo, para la implementación mostrada en
  el programa~\ref{pro:metodo-puntofijo} del
  Algoritmo~\ref{alg:metodo-punto_fijo} aplicada a $g_2$ se obtiene,
  partiendo de $x_0=1$, el siguiente resultado:
  \pythonexternal{tema1/src/puntofijo-test1.py}
  \begin{pythonoutput}
    \pythonexternal[backgroundcolor=\color{white},title={Resultado}]{tema1/src/puntofijo-test1.out}
  \end{pythonoutput}    
  \label{ex:punto-fijo-1}
  \begin{program}
    \widepythonexternal{tema1/src/puntofijo.py}
    \label{pro:metodo-puntofijo}
    \caption{Una implementación en lenguaje Python del método de
      aproximaciones sucesivas para el cálculo un punto fijo $x=g(x)$}
  \end{program}
\end{example}

Para terminar, mostramos un resultado que garantiza la convergencia
de~(\ref{eq:MAS}) hacia un punto fijo conocido, sin utilizar hipótesis
globales sobre el intervalo. En concreto, en el siguiente teorema
solamente se utilizan hipótesis locales sobre la derivada de $g$.

\begin{theorem}[Convergencia local de los métodos de punto fijo]
  \label{thm:punto-fijo-convergencia-local}
  Sea $\cero$ un punto fijo de una función $g$. Supongamos que existe
  $\delta>0$ tal que $g\in C^1(\cero-\delta,\cero+\delta)$ y que
  $|g'(\cero)|<1$. Entonces:
  \begin{enumerate}
  \item Existe $\rho\in (0,\delta)$ tal que método de punto
    fijo~(\ref{eq:MAS}) está bien definido y converge hacia $\cero$
    (para cualquier inicialización $x_0 \in [x_0-\rho,x_0+\rho]$).
  \item Existe una constante de contractividad $\lambda\in [0,1)$ (que
    depende de $\rho$ y $|g'(\cero)|$) para la que se verifican las
    estimaciones a priori~(\ref{eq:pto-fijo:cota-a-priori}) y a
    posterior~(\ref{eq:pto-fijo:cota-a-posteriori}) en
    $[x_0-\rho,x_0+\rho]$.
  \end{enumerate}
  \begin{proof}
    Simplemente tenemos que aplicar el
    teorema~\ref{thm:punto-fijo-Banach} (Teorema del punto fijo) en un
    intervalo adecuado $[x_0-\rho,x_0+\rho]$. En concreto, como
    $g'(\cero)<1$ y $g \in C^1(\cero-\delta,\cero+\delta)$, podemos
    asegurar que para algún $\rho\in(0,\delta)$,
    $$
    |g'(x)|<1 \quad \forall x\in [\cero-\rho, \cero+\rho],
    $$
    luego $g$ es contractiva en $[\cero-\rho, \cero+\rho]$ (debido a
    la proposición~\ref{pro:1} y a la observación~\ref{rk:3}), con
    constante $\lambda<1$.

    Además es fácil ver que $g([\cero-\rho, \cero+\rho]) \subset
    [\cero-\rho, \cero+\rho]$, pues si $x\in[\cero-\rho, \cero+\rho]$
    se tiene $ |x-\cero|<\rho$ y por tanto
    $$ 
    |g(x)-\cero| = |g(x)-g(\cero)| \le \lambda |x-\cero|< \rho
    \Rightarrow g(x)\in [\cero-\rho, \cero+\rho].
    $$    
  \end{proof}
\end{theorem}

\section{El método de Newton}
\label{sec:metodo-de-newton}
\begin{figure}
  % Código Python para calcular los datos de las rectas tangentes
  % f = lambda x: x+x**2+x**5
  % df = lambda x: 1+2*x+5*x**4
  % x0=0.85; y0=f(x0); r0 = lambda x: y0+(x-x0)*df(x0)
  % print "Punto 0:", (x0, y0, df(x0))
  % x1=x0-y0/df(x0); y1=f(x1); r1 = lambda x: y1+(x-x1)*df(x1)
  % print "Punto 1:", (x1, y1, df(x1))
  % x2=x1-y1/df(x1); y2=f(x2); r2 = lambda x: y2+(x-x2)*df(x2)
  % print "Punto 2:", x2
  \begin{graficaTikz}[width=25em, height=17em]
    \def\Ax{0.85} \def\Ay{ 2.0162053125} \def\mA{5.31003125}
    \def\Bx{0.47030255236256846} \def\By{0.7144954568706967} \def\mB{2.185217999486167}
    \def\Cx{0.143334965129}
    \begin{axis}[\axisXYmiddle, 
      restrict y to domain=-1:3.5,
      legend pos = outer north east, legend cell align=left,
      ticks=none]
      % Draw a curve
      \addplot[domain=-0.2:1.1, blue, ultra thick, samples=40]
      {x+x^2+x^4};
      % Tangent 1
      \addplot[domain=-0.2:1.1, gray, thick, samples=40]
      {\Ay+(x-\Ax)*\mA};
      \node[coordinate, medium dot]  at (axis cs:\Ax,\Ay) {};
      \addplot[dashed] coordinates {(\Ax,0) (\Ax,\Ay)};
      \node[coordinate, medium dot, pin=-45:{$x_0$}]
      at (axis cs:\Ax,0) {};
      % Tangent 2
      \addplot[domain=-0.2:1.1, darkgreen, thick, samples=40]
      {\By+(x-\Bx)*\mB};
      \node[coordinate, medium dot]  at (axis cs:\Bx,\By) {};
      \addplot[dashed] coordinates {(\Bx,0) (\Bx,\By)};
      \node[coordinate, medium dot, pin=-45:{$x_1$}] 
      at (axis cs:\Bx,0) {};
      \legend {$y=f(x)$};
      % Point 3
      \node[coordinate, medium dot, pin=-45:{$x_2$}] 
      at (axis cs:\Cx,0) {};
    \end{axis}
  \end{graficaTikz}
  \caption{Interpretación geométrica del método de Newton}
  \label{fig:newton-interpretacion-geometrica}
\end{figure}
El método de Newton (también conocido como método de Newton-Raphson o
de la tangente) es uno de los métodos numéricos más conocidos y más
eficientes para el cálculo de raíces, si bien las condiciones para la
convergencia son más restrictivas que en métodos anteriores. 

El método fue deducido históricamente a partir de la siguiente
construcción geométrica
(figura~\ref{fig:newton-interpretacion-geometrica}): Dada
$f:[a,b]\to\Rset$ continua en $[a,b]$ y derivable en $(a,b)$ y dado
$x_0\in [a,b]$, para cada $k\ge 0$ calculamos $x_{k+1}$ como la
abscisa de la intersección del eje $y=0$ con la recta tangente a $f$
en $(x_k,f(x_k))$. Como esta recta tangente viene dada por
$y-f(x_k) = f'(x_k)(x-x_k)$, el \resaltar{método de
  Newton} se formula de la siguiente forma:
\begin{equation}
  \tag{$M_{\text{N}}$}
  \left\{
    \begin{array}{l}
      \text{Dado } x_0\in [a,b], \\ \noalign{\medskip}
      \text{calcular } x_{k+1} = x_k - \dfrac{f(x_k)}{f'(x_k)} , \quad \forall k\ge 0.
    \end{array}
  \right.
  \label{eq:MetNewton}
\end{equation}
Obsérvese que para que el método de Newton esté bien definido se debe
imponer la siguiente restricción: $f'(x)\neq 0$ para todo $x\in
[a,b]$. Es un método cuya implementación es sencilla (véase el
algoritmo~\ref{alg:metodo-newton}) siempre y cuando sea conocida la
derivada de la función $f$.
\begin{algorithm}  \begin{python}
def newton(f, df, x0, tol, max_iters):
    iter = 0
    while iter < max_iters:
        x1 = x0 - f(x0)/df(x0)
        if abs(x1-x0) < tol: 
            return x1
        else:
            x0 = x1
            iter = iter + 1        
    print "Fallo de convergencia en el método de Newton!"
\end{python}
\caption{Método de Newton}
\label{alg:metodo-newton}
\end{algorithm}
Además de su interpretación geométrica, existen otras formas de
introducir el método de Newton:
\begin{enumerate}
\item Mediante el \textbf{desarrollo de Taylor} de $f$: dado $x_n\in [a,b]$,
  \begin{align*}
    f(x)&=f(x_n) + f'(x_n)(x-x_n) + \frac 12 f''(\xi_n)(x-x_n)^2, \quad
    \text{con $\xi_n$ entre $x$ y $x_n$}.
    \\
    \intertext{En particular, para $x=\cero$ se tiene $f(\cero)=0$,
      luego}
    0=f(\cero)&=f(x_n) + f'(x_n)(\cero-x_n) + \frac 12 f''(\xi_n)(\cero-x_n)^2.
  \end{align*}
  Suponiendo que $x_n$ sea ``una buena aproximación'' de $\cero$ el
  término $|\cero-x_n|$ es ``pequeño'', luego $(\cero-x_n)^2$ es
  ``mucho más pequeño'' y puede eliminarse el último sumando de la
  ecuación anterior, de donde
  \begin{equation*}
        0 \approx f(x_n) + f'(x_n)(\cero-x_n), \quad \text{es decir} \quad
        \cero \approx x_n - \frac{f(x_n)}{f'(x_n)},
  \end{equation*}
  lo cual justifica que la elección de $x_{n+1}$ dada
  en~(\ref{eq:MetNewton}) es una aproximación de $\cero$.
\item Como una técnica de \textbf{punto fijo} en la que se impone
  orden al menos cuadrático. % La idea es reescribir~(\ref{eq:raiz})
  % en la forma~(\ref{eq:punto-fijo}).
  Para ello, observamos dada $h:[a,b]\to\Rset$ tal que $h(x)\neq 0$
  para toda $x\in[a,b]$,
  \begin{align*}
    x \text{ es solución de~(\ref{eq:raiz})} \ \Leftrightarrow\ & h(x)f(x)=0 
    \ \Leftrightarrow\ x-h(x)f(x)=x \\
    \ \Leftrightarrow\ & x \text{ es solución de~(\ref{eq:punto-fijo}) para }
    g(x)=x-h(x)f(x).
  \end{align*}
  La idea del método de Newton consiste en elegir $h$ de forma que
  $g'(\cero)=0$, lo que (asumiendo la regularidad necesaria) implicaría
  que el método de punto fijo~(\ref{eq:MAS}) para $g$ tiene orden (al
  menos) dos (véase la observación~\ref{rk:MAS.orden.p}). Por
  definición de $g$,
  $$
  0 = g'(\cero) = 1-h'(\cero)f(\cero)-h(\cero)f'(\cero)
  $$
  y como $f(\cero)=0$ nos queda 
  $$
  0 = 1-h(\cero)f'(\cero).
  $$
  Es decir, la mejor elección es $h(x)=1/f'(x)$, con lo que
  $g(x)=x-f(x)/f'(x)$. El método de Newton no es más
  que un método de punto fijo~(\ref{eq:MAS}) aplicado a esta función.
\end{enumerate}

Esta última justificación de~(\ref{eq:MetNewton}) en términos de un
método de punto fijo es la más rigurosa y de hecho nos permite aplicar
toda la artillería desarrollada en la
sección~\ref{sec:metodos-de-punto-fijo}. El inconveniente es que, de
cara a la existencia y unicidad de solución, el
teorema~\ref{thm:punto-fijo-Banach} (Teorema de punto fijo) impone
condiciones que no son fáciles de verificar.  El siguiente resultado
ofrece una condiciones suficientes más apropiadas, de tipo global (en
todo el intervalo).

\begin{theorem}[Convergencia global del método de Newton]
  Sea $f\in C^2([a,b])$ para la que se verifican las siguientes
  hipótesis:
  \begin{enumerate}[label=($N_{\arabic*}$)]
  \item $f(a)f(b)<0$.
    \label{item:Newton.H1}
  \item  $f'(x)\neq 0$, para todo $x \in [a,b]$.
    \label{item:Newton.H2}    
  \item 
    El signo de $f''$ no cambia en $[a,b]$ (o sea,
      $f''(x)\ge 0 \ \forall x\in [a,b]$ o $f''(x)\le 0 \ \forall x\in [a,b]$).
    \label{item:Newton.H3}
  \item
    $\max\left\{ \dfrac{|f(a)|}{|f'(a)|}, \dfrac{|f(b)|}{|f'(b)|}
    \right\} \le b-a.$
    \label{item:Newton:H4}
  \end{enumerate}
  Entonces:
  \begin{enumerate}
  \item $f$ tiene una única raíz, $\alpha$, en $[a,b]$.
  \item (\ref{eq:MetNewton}) está bien definido (es decir, para todo
    $x_0\in [a,b]$ se tiene que $\{x_k\} \subset [a,b]$).
  \item (\ref{eq:MetNewton}) es convergente (para todo $x_0\in [a,b]$)
    y $\lim x_k = \cero$.
  \item El orden de (\ref{eq:MetNewton}) es, al menos, cuadrático. En
    concreto, para todo $k\ge 0$,
    \begin{equation}
      |x_{x+1} - \cero| \le \frac {M''}{2m'} |x_k-\alpha|^2, 
      \label{eq:Newton.orden.2}    
    \end{equation}
    donde definimos $m':=\min_{x\in[a,b]} |f'(x)|$ y $M'':=\max_{x\in[a,b]}|f''(x)|$.
  \end{enumerate}
  \label{thm:Newton.convergencia.global}  
\end{theorem}

\begin{proof}
  Como $f'$ es continua y la hipótesis~\ref{item:Newton.H2} especifica
  que $f'(x) \neq 0$, tendremos que $f'(x)>0$ o $f'(x)<0$ en $[a,b]$.
  Además, según la hipótesis~\ref{item:Newton.H3}, $f''(x)\ge 0$ o
  $f''(x)\le 0$ en $[a,b]$. En esta demostración supondremos que
  \begin{equation}
    f'(x)>0 \quad \text{y}\quad f''(x)\le 0
    \label{eq:f'>0.f''<=0}
  \end{equation}
  (es decir, $f$ es creciente y convexa, como en la
  figura~\ref{fig:newton-interpretacion-geometrica}). Para las otras
  tres combinaciones posibles ($f'>0$ y $f''\ge 0$, $f'<0$ y $f''\ge 0$,
  $f'<0$ y $f''\le 0$) la demostración es similar a la actual.  
  En el caso~\eqref{eq:f'>0.f''<=0}, el hecho de que $f$ sea creciente implica que
  \begin{equation*}
    f(a)<0 \quad\text{y}\quad f(b)>0.
  \end{equation*}
  Y como además $f'$ es decreciente (pues $f''<0$), la
  hipótesis~\ref{item:Newton:H4} se puede escribir de forma más
  sencilla:
  \begin{equation}
    \frac{f(b)}{f'(b)} < b-a.
    \label{eq:Newton:H4.simplificada}
  \end{equation}
  A continuación, probaremos consecutivamente los cuatro puntos del
  teorema.

  \punto{1} % --------------------------------------------------
  Es evidente que existe al menos una raíz, $\cero$, pues la
  hipótesis~\ref{item:Newton.H1} implica que podemos aplicar el
  Teorema de Bolzano en $[a,b]$. Más aún, como $f'(x)>0$, la raíz es única (véase el
  Corolario~\ref{cor:tema1:exist+unic}).

  \punto{2} % --------------------------------------------------
  El método de Newton es un
  método~(\ref{eq:MAS}), $x_{k+1}=g(x_k)$, para la siguiente función:
  \begin{equation*}
    g(x)=x-\frac{f(x)}{f'(x)}.
  \end{equation*}
  Para que este tipo de métodos estén bien definidos (o sea
  $\{x_k\}\subset [a,b]$) es suficiente que $g([a,b])\subset [a,b]$.
  % Veremos, en concreto, que
  % \begin{equation}
  %   g([a,b])\subset [a,\cero]\subset [a,b].
  %   \label{eq:3}
  % \end{equation}
  Para ello, estudiamos el crecimiento de $g$.
  Como $f\in C^2([a,b])$ y $f'(x)\neq 0$ en
  $[a,b]$, deducimos que $g\in C^1([a,b])$ y
  \begin{equation}
    g'(x)
    = 1-\frac{\left(f'(x)\right)^2-f(x)f''(x)}{\left(f'(x)\right)^2}
    = \frac{f(x)f''(x)}{\left(f'(x)\right)^2}.
    \label{eq:Newton.g'(x)}    
  \end{equation}
  Debido a que $f''(x)\le 0$, el signo de $g'(x)$ depende sólo del
  signo de $f(x)$. Así
  \begin{itemize}%[label=\emph{\roman*)}]
  \item Si $x\in [a,\cero]$, $g'(x)\ge 0$, es decir $g$ es creciente en
    $[a,\cero]$.
  \item Si $x\in [\cero,b]$, $g'(x)\le 0$, esto es, $g$ es decreciente
    en $[\cero,b]$.
  \end{itemize}

  Por lo tanto, $\max_{x\in [a,b]} g(x)=g(\alpha)$, lo que implica que
  para todo
  $x\in [a,b]$ se tiene 
  $$
  g(x)\le g(\alpha)=\alpha \le a.
  $$
  Veamos que también $g(x)\ge a$ en $[a,b]$ y así concluimos que
  $g([a,b]) \subset [a,b]$, más en concreto 
  \begin{equation}
    g([a,b])\subset [a,\alpha] \subset [a,b].\label{eq:2}    
  \end{equation}

  \begin{itemize}
  \item Si $x\in [a,\cero]$, como $g$ es creciente en este intervalo,
    $$g(x)\ge  g(a) = a-f(a)/f'(a) > a.$$
  \item Si $x\in [\cero,b]$ no basta usar que $g$ es
    decreciente en el intervalo: necesitamos además la
    hipótesis~\ref{item:Newton:H4} que, como vimos, se escribe
    como~(\ref{eq:Newton:H4.simplificada}). Así
    $$g(x) \ge g(b)=b-f(b)/f'(b) \ge b-(b-a)=a.$$
  \end{itemize}
  Así hemos visto que se verifica~\eqref{eq:2}.

  \punto{3} % --------------------------------------------------
  Veremos que la sucesión $\{x_k\}$ es convergente y su límite es
  $\alpha$ a través del siguiente razonamiento: \textit{(a)} $\{x_k\}$
  está acotada superiormente, \textit{(b)} $\{x_k\}$ es creciente (a
  partir del segundo término), luego existe $\lim x_k=\overline\cero$
  y \textit{(c)} $\overline\cero$ coincide con $\cero$.
  \begin{enumerate}[label=\emph{(\alph*)}]
  \item Para cualquier $x_0\in[a,b]$, consideremos la sucesión
    $\{x_k\}$ definida por~\eqref{eq:MetNewton}. Para cualquier $k\ge
    0$, $x_{k+1}=g(x_{k})$ y como $g([a,b])\subset [a,\cero]$
    (según vimos en~\eqref{eq:2}) tenemos que
    $x_{k+1}\in [a,\alpha]$.
  \item Para ver que la sucesión es creciente a partir del segundo
    término basta darse cuenta de que $f(x_k)\le 0$ para todo $k\ge 1$
    (no necesariamente para $k=0$), pues $x_k=g(x_{k-1})\in
    [a,\alpha]$. Por tanto
    $$
    x_{k+1}=g(x_k)=x_k - \frac{f(x_k)}{f'(x_k)} \ge x_k.
    $$
  \item Sea entonces $\overline\alpha = \lim x_k$. Tomando límite en cada
    término de la igualdad
    \begin{equation*}
      x_{k+1}=x_k- \frac{f(x_k)}{f'(x_k)}
%    \end{equation*}
    \quad\text{ llegamos a }\quad
%    \begin{equation*}
      \overline\cero = \overline\cero -
      \frac{f(\overline\cero)}{f'(\overline\cero)},
    \end{equation*}
    por lo que $f(\overline\cero)=0$. Luego $\overline\cero=\cero$ (la
    raíz de $f$ es única).
  \end{enumerate}

  \punto{4} % --------------------------------------------------
  Para estudiar el orden de convergencia, aunque $g'(\cero)=0$ (debido
  a~\eqref{eq:Newton.g'(x)}), no podemos aplicar la
  observación~\ref{rk:MAS.orden.p} debido a que, en principio,
  $g\not\in C^2([a,b])$. Pero sí podemos contar con $f\in C^2([a,b])$,
  por lo que tenemos siguiente desarrollo de Taylor:
  \begin{equation*}
    0 = f(\cero)=f(x_k)+f'(x_k)(\cero-x_k)+\frac12 f''(\theta_k)(\cero-x_k)^2,
    \quad\forall k\ge 0.
  \end{equation*}
  Por otra parte, por construcción de $x_{k+1}$ en~\eqref{eq:MetNewton},
  \begin{equation*}
    0=f(x_k)+f'(x_k)(x_{k+1}-x_k).
  \end{equation*}
  Restando ambas expresiones,
  \begin{equation*}
    f'(x_k)(\cero-x_{k+1}) + \frac12 f''(\theta_k)(\cero-x_k)^2 = 0.
  \end{equation*}
  Y despejando $\cero-x_{k+1}$ y tomando valor absoluto
  deducimos~\eqref{eq:Newton.orden.2}, lo que finaliza la demostración.
\end{proof}

\begin{remark}~
  \label{rk:4}
  \begin{enumerate}
  \item Además de la estimación ``a
    priori''~\eqref{eq:Newton.orden.2}, se tiene (no escribiremos aquí
    la demostración, baste decir que se basa en un desarrollo de
    Taylor de orden $2$ entre $x_k$ y
    $x_{k+1}$) la siguiente estimación ``a posteriori'':
    \begin{equation}
      |x_{k+1}-\cero| \le \frac{M''}{2m'}|x_{k+1}-x_k|.
      \label{eq:newton.estimac.a.posteriori}
    \end{equation}
    Gracias a ella, si en el algoritmo de Newton realizamos
    iteraciones hasta que $|x_{k+1}-x_k|$ sea pequeño, tenemos
    garantizada una buena aproximación de la solución exacta. Por
    ejemplo, dado $\varepsilon>0$, si hacemos que 
    $$
    |x_{k+1}-x_k| < \frac{M''}{2m'} < \varepsilon
    \quad\text{entonces}\quad  |x_{k+1}-\cero| < \varepsilon.
    $$ 
  \item En la demostración del Teorema anterior hemos probado que la
    sucesión $\{x_k\}$ es monótona a partir del primer segundo
    término, $x_1$. En concreto, en el caso estudiado en la
    demostración ($f'>0$ y $f''\le 0$) hemos visto que la sucesión es
    monótona creciente. Se deja como ejercicio al lector el discutir
    el crecimiento/decrecimiento en los otros tres casos posibles.
  \item La hipótesis~\ref{item:Newton:H4} es necesaria para (como
    hemos visto en la demostración) garantizar que el método de Newton
    está bien definido en el sentido de los métodos de punto fijo
    ($g([a,b]\subset [a,b]$). En realidad, es sólo $x_1$ (en la
    primera iteración) el punto que podría quedar fuera de $[a,b]$
    pues, como hemos visto, el resto de la sucesión converge
    monótonamente hacia $\cero$.

    El siguiente resultado proporciona una condición muy útil para
    evitar la hipótesis~\ref{item:Newton:H4}:
  \end{enumerate}
\end{remark}

\begin{corollary}[Regla de Fourier]
  Sea $f \in C^2([a,b])$ una función que satisface las
  hipótesis~\ref{item:Newton.H1}--\ref{item:Newton.H3} del
  Teorema~\ref{thm:Newton.convergencia.global}. Si elegimos $x_0=a$
  o $x_0=b$ de forma que se verifique
  \begin{equation}
    \label{eq:4}
    f(x_0) f''(x_0) > 0,
  \end{equation}
  entonces $f$ tiene una única raíz $\alpha$ en $[a,b]$ y la
  sucesión $\{x_k\}$ definida por~\eqref{eq:MetNewton} está bien
  definida, converge monótonamente hacia $\cero$ y se verifican las
  acotaciones~\eqref{eq:Newton.orden.2}
  y~\eqref{eq:newton.estimac.a.posteriori}.
  \label{cor:regla.fourier}    
  \end{corollary}
  \begin{proof}
    Consideremos de nuevo $f'>0$ y $f''<0$ en $[a,b]$ (los otros tres
    casos son similares).  Como hemos visto en la demostración del
    Teorema~\ref{thm:Newton.convergencia.global}, las
    hipótesis~\ref{item:Newton.H1}--\ref{item:Newton.H3} implican que
    existe una sola raíz de $f$ y que la función de punto fijo, $g$,
    verifica $g([a,b])\subset [a,\alpha]$.
    
    En el caso $f'>0$ y $f''<0$ tenemos que $f(a)<0$ y por tanto
    elegimos $x_0=a$ para que se verifique~\eqref{eq:4}. Esto
    significa que $x_0\in [a,\alpha]$ y, razonando como anteriormente, toda
    la sucesión $\{x_k\}_{k\ge 0} \subset[a,\alpha]$ y es creciente y
    converge hacia $\cero$. La
    estimación~\eqref{eq:Newton.orden.2} se puede demostrar
    exactamente igual que antes
    y~\eqref{eq:newton.estimac.a.posteriori} se deja sin demostrar,
    como en la observación~\ref{rk:4}.
  \end{proof}
  
  \begin{example}
    Consideremos la siguiente ecuación: 
    $$f(x)=2\log(x)-x+1=0.$$
  \begin{figure}
    \label{fig:tema1:ejemplo-separ-soluc-1}
    \begin{graficaTikz}[width=20em, height=13em]
      \begin{axis}[\axisXYmiddle]
        % Draw a curve
        \addplot[domain=0.6:4, blue, ultra thick, samples=40] {2*ln(x)-(x-1)};
        \node[coordinate, medium dot, pin=-80:{$\cero_1$}] at (axis cs:1,0) {};
        \def\Bx{3.5128} 
        \node[coordinate, medium dot, pin=-100:{$\cero_2$}] at (axis
        cs:\Bx,0) {};
%        \legend {$f(x)$}
      \end{axis}
    \end{graficaTikz}
    \caption{Gráfica en la que se muestran los dos ceros de $f(x)=2\log(x)-x+1$}
    \label{fig:2log.x.-x+1}
  \end{figure}  
  La función, cuya gráfica se representa en la
  figura~\ref{fig:2log.x.-x+1}, es continua y derivable en su dominio,
  $(0,+\infty)$. Su derivada, $f'(x)=(2-x)/x$, está bien definida en
  $(0,+\infty)$ y tiene un único cero, $x=2$ (siendo $f'>0$ en $(0,2)$
  y $f'<0$ en $(2,+\infty)$). Por lo tanto, $f$ tiene a lo sumo dos
  ceros, separados por $x=2$, tal y como sugiere la gráfica. El
  primero de estos ceros es, evidentemente, $\cero_1=1$ (pues
  $f(1)=0$) y el segundo, $\cero_2$, parece estar entorno a
  $x=3.5$. Gracias al teorema de Bolzano podemos asegurar que está
  localizado en $[3,4]$, pues $f(3)=0.197...>0$ y $f(4)=-0.227...<0$.
  
  Utilizaremos el método de Newton para calcular $\cero_2$,
  comprobando previamente que en el intervalo $[3,4]$ se verifican las
  hipótesis de la regla de Fourier
  (corolario~\ref{cor:regla.fourier}).
  \begin{itemize}
  \item $f\in C^2([3,4])$, de hecho $f''(x)=-2/x^2 < 0$ en $[3,4]$.
  \item Se verifican~\ref{item:Newton.H1}, es decir $f(3)f(4)<0$,
    \ref{item:Newton.H2}, de hecho $f'<0$ en $[3,4]$
    y~\ref{item:Newton.H3}, en concreto $f''<0$ en $[3,4]$.
  \item Si elegimos $x_0=4$, tenemos $f(x_0)f''(x_0)>0$.
  \end{itemize}
  Así tenemos asegurada la convergencia monótona de $\{x_k\}$ hacia
  $\cero_2$, a partir de $x_0=4$ (en este caso, de forma decreciente).
  
  Con la ayuda de una calculadora o un ordenador es fácil realizar
  algunas iteraciones del método de Newton. Por ejemplo, utilizando la
  implementación que aparece en el programa~\ref{pro:metodo-newton},
  se obtiene el siguiente resultado (en el que conseguimos
  $|x_{k+1}-x_k|<10^{-6}$ en tan solo 4 iteraciones):
  \pythonexternal{tema1/src/newton-test1.py}
  \begin{pythonoutput}
    \pythonexternal[backgroundcolor=\color{white},title={Resultado}]{tema1/src/newton-test1.out}
  \end{pythonoutput}    
  \begin{program}
    \widepythonexternal{tema1/src/newton.py}
    \label{pro:metodo-newton}
    \caption{Una implementación en lenguaje Python del método de
      Newton}
  \end{program}

  En este caso, tenemos (debido a que $f'$ es creciente y $f''$
  decreciente en $[3,4]$):
  \begin{equation*}
    \left.
    \begin{array}{ll}
      M''&:=\displaystyle\max_{x\in[3,4]}|f''(x)|=|f''(3)|=2/9
      \\\noalign{\medskip}
      m'&:=\displaystyle\min_{x\in[3,4]} |f'(x)|=|f'(3)|=1/3
    \end{array}
    \right\} \ \Rightarrow \
    \frac{M''}{2m'} = 3.
  \end{equation*}
  La estimación a
  posteriori~\eqref{eq:newton.estimac.a.posteriori} nos permite acotar el
  error absoluto para tolerancia $10^{-6}$,
  \begin{align*}
    |x_{4}-\cero|&\le \frac{M''}{2m'} |x_4-x_3| \le 3\cdot 10^{-6},
    \\
    \intertext{o incluso afinar más, usando el resultado del programa anterior,}
    |x_{4}-\cero|&\le \frac{M''}{2m'} |x_4-x_3| \le 3 \cdot 2.28588
    \cdot 10^{-7}
    = 6.85764 \cdot 10^{-7}.
  \end{align*}
\end{example}

\section{Los métodos de la secante y de regula falsi}
\label{sec:secante-y-regula-falsi}

\subsection{Método de la secante}
\label{sec:metodo-secante}

El método de Newton tiene como inconveniente la necesidad del cálculo
de la derivada y la evaluación de $f'(x)$ en cada iteración, lo cual
no podría no ser posible o bien requerir un esfuerzo de cálculo
prohibitivo. Por ejemplo la función $f$ podría venir dada por una
tabla de valores sobre un número discreto de puntos, o bien dada por
un programa de ordenador para el que no pudiera contemplarse la
derivada.

\begin{figure}
  \begin{graficaTikz}[width=25em, height=16em]
    \def\Ax{0.6} \def\Ay{1.35}
    \def\Bx{0.3} \def\By{0.41}
    \def\Cx{0.166}
    \begin{axis}[\axisXYmiddle, 
      restrict y to domain=-0.6:1.9,
      legend pos = outer north east, legend cell align=left,
      ticks=none]
      \legend {$y=f(x)$};
      % Draw a curve
      \addplot[domain=-0.1:1.15, blue, ultra thick, samples=40]
      {x+x^2+3*x^4};
      % Secant
      \addplot[domain=-0.2:1.15, gray, thick]
      {\Ay+(x-\Ax)*(\Ay-\By)/(\Ax-\Bx)};
      % Points
      \node[coordinate, medium dot, pin=-45:{$x_{k-1}$}]  at (axis cs:\Ax,0) {};
      \node[coordinate, medium dot]  at (axis cs:\Ax,\Ay) {};
      \addplot[dashed] coordinates {(\Ax,0) (\Ax,\Ay)};

      \node[coordinate, medium dot, pin=-45:{$x_k$}]  at (axis cs:\Bx,0) {};
      \node[coordinate, medium dot]  at (axis cs:\Bx,\By) {};            
      \addplot[dashed] coordinates {(\Bx,0) (\Bx,\By)};

      \node[coordinate, medium dot, pin=-45:{$x_{k+1}$}] at (axis cs:\Cx,0) {};
    \end{axis}
  \end{graficaTikz}
  \caption{Interpretación geométrica del método de la secante}
  \label{fig:secante-interpretacion-geometrica}
\end{figure}
El método de la secante es una variante del método de Newton en el que
se evita el cálculo de $f'$ utilizando la siguiente idea: dadas dos
aproximaciones $x_{k-1}$ y $x_k$, se construye una nueva aproximación
$x_{k+1}$ como la abscisa de la intersección del eje $y=0$ con la
recta secante a la curva $y=f(x)$ que pasa por los puntos
$(x_{k-1},f(x_{k-1}))$ y $(x_k,f(x_k))$. Como esta recta secante viene
dada por
$$
y-f(x_k) = \frac{f(x_{k})-f(x_{k-1})}{x_k-x_{k-1}}(x-x_k),
$$
tomando $y=0$ y $x=x_{k+1}$ tenemos la formulación del \resaltar{método de la secante}:
\begin{equation}
  \tag{$M_{\text{S}}$}
  \left\{
    \begin{array}{l}
      \text{Dados } x_0,x_1\in [a,b], \\ \noalign{\medskip}
      \text{calcular } x_{k+1} = x_k - f(x_k)
      \dfrac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})},
      \quad \forall k\ge 1.
    \end{array}
  \right.
  \label{eq:MetSecante}
\end{equation}

\begin{algorithm}  \begin{python}
def secante(f, x0, x1, tol, max_iters):
    iter = 0
    while iter < max_iters:
        x2 = x1 - f(x1)* (x1-x0)/(f(x1)-f(x0))
        if abs(x2-x1) < tol: 
            return x2
        else:
            x0 = x1
            x1 = x2
            iter = iter + 1        
    print "Fallo de convergencia en el método de la secante!"
\end{python}
\caption{Método de la secante}
\label{alg:metodo-secante}
\end{algorithm}
El método de la secante, formulado en el
algoritmo~\ref{alg:metodo-secante}, puede ser también deducido a
partir del método de Newton, a través de la siguiente aproximación:
\begin{equation*}
  f'(x_k) \approx  \frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})}.
\end{equation*}
Se trata de un método de dos pasos (para el cálculo de $x_{k+1}$ se
utilizan dos estimaciones previas, $x_k$ y $x_{k-1}$) que tiene orden
de convergencia superlineal. Concretamente, se puede demostrar%
\footnote{\textsf{Bibliografía}: Kincaid, D., Cheney, W., Análisis
  Numérico. \textit{Las matemáticas del cálculo
    científico}. Addison-Wesley-Iberoamericana 1994.}  que el método
de la secante tiene orden $\phi$, donde
$$
\phi=\frac{1 + \sqrt 5}{2} \approx 1.62
$$
es el número de oro. Aunque el orden de convergencia sea menor que en
el método de Newton, el método de bisección tiene la ventaja adicional
de necesitar una sola evaluación de $f$ en cada iteración (siempre y
cuando se implemente cuidadosamente el algoritmo~\ref{alg:metodo-secante}).

\subsection{Método de \emph{regula-falsi}}
\label{sec:metodo-de-regula-falsi}

El método de la falsa posición o \emph{regula falsi} es una variante del
método de bisección que incorpora ciertas características del método
de la secante. En concreto, se sustituye el cálculo del punto medio de
los subintervalos $[a_k,b_k]$ por el punto que proporciona el método
de la secante con $x_{k-1}=a_k$ y $x_k=b_k$.

Supongamos que $f$ tiene una única solución en
$[a,b]$, siendo $f(a)f(b)<0$.  Construimos una sucesión decreciente de
intervalos, del tipo~\eqref{eq:tema1:bisecc:0}, definida a partir de
puntos interiores dados por el método de la secante, tal y como sigue:
\begin{itemize}
\item Como inicialización, fijamos $a_0=a$ y $b_0=b$.
\item En la etapa $k+1$:
  \begin{enumerate}%[label=\textit{\arabic*)}]
  \item 
    Calculamos $x_{k+1}$ usando~\eqref{eq:MetSecante}, donde $x_{k-1}$
    y $x_k$ vienen dados por los extremos del intervalo
    $[a_k,b_k]$. Es decir:
    \begin{equation*}
      x_{k+1} = b_k - f(b_k) \dfrac{b_k-a_k}{f(b_k)-f(a_k)}.
    \end{equation*}
    Geométricamente, $x_{x+1}$ es la abscisa de la
    intersección del eje $y=0$ con la secante a $y=f(x)$ que pasa por
    los puntos $(a_k,f(a_k))$ y $(b_k,f(b_k))$.
  \item Definimos $a_{k+1}$ y $b_{k+1}$ de forma que $f$ cambie de signo en
    $[a_{k+1},b_{k+1}]$:
    \begin{enumerate}
    \item Si $f(a_{k})f(x_{k+1})<0$, elegimos $[a_{k+1},b_{k+1}]=[a_{k}, x_{k+1}]$.
    \item Si $f(x_{k+1})f(b_{k})<0$, elegimos $[a_{k+1},b_{k+1}]=[x_{k+1}, b_{k+1}]$.
    \end{enumerate}
  \end{enumerate}
\end{itemize}

El método de \textit{regula falsi} es un método de dos pasos. Se puede
demostrar que converge con orden lineal. En el
algoritmo~\ref{alg:metodo-regula-falsi} se recogen algunas nociones
sobre su implementación; como se puede observar, es muy similar al
algoritmo~\ref{alg:metodo-biseccion} (método de bisección) teniendo en
cuenta que:
\begin{itemize}
\item Se cambia la condición \texttt{c = (b-a)/2} por \texttt{c = b -
    f(b)*(b-a)/(f(b)-f(a))}.
\item Se modifica ligeramente el test de parada (para tener en
  cuenta que todas las iteraciones se pueden estar realizando lejos de
  uno de los extremos del intervalo).
\end{itemize}


\begin{algorithm}  \begin{python}
def regula_falsi(f, a, b, tol, max_iters):
    iter = 0
    while iter < max_iters:
        c = b - f(b)*(b-a)/(f(b)-f(a))
        if min(b-c, c-a) < tol:
            return c
        else:
            if f(a)*f(c) < 0:
                b=c
            else:
                a=c
        iter = iter + 1
    print "Fallo de convergencia en el método de 'regula-falsi'"
\end{python}
\caption{Método de \textit{regula falsi}}
\label{alg:metodo-regula-falsi}
\end{algorithm}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../apuntes-MNII.tex"
%%% End: 
